{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbofDRp5ZAN",
        "outputId": "be19d1ce-f53f-4361-bf40-012c8260a338"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import nltk as nl\n",
        "from tqdm import tqdm  \n",
        "nl.download('punkt')\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUXSU0AST-r8"
      },
      "source": [
        "The implementation of Transformer is refered to [https://github.com/qingyujean/Magic-NLPer/tree/main/NLP/MachineTranslation/transformer/pytorch] and [https://tensorflow.google.cn/tutorials/text/transformer]. The decription for each part is follow the tutoral of tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHKjbDgD70-B"
      },
      "source": [
        "# tokenizer = lambda x: x.split() # word-level\n",
        "tokenizer = lambda x: list(x) # char-level"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbdeQeBAueI"
      },
      "source": [
        "p_intent = []\n",
        "p_snippet = []\n",
        "c = 0\n",
        "iit = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent:\n",
        "        iit.append(c)\n",
        "      c+=1\n",
        "      p_intent.append(i[:-1].lower())\n",
        "p_intentn = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentn.append(i[:-1].lower())\n",
        "with open('y.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippet.append(i.lower())\n",
        "  \n",
        "train_pairs = [[p_intentn[i],p_snippet[i]] for i in iit]\n",
        "p_intentval = []\n",
        "p_snippetval = []\n",
        "iiv = []\n",
        "c= 0\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intentval:\n",
        "        iiv.append(c)\n",
        "      \n",
        "      p_intentval.append(i[:-1].lower())\n",
        "      c+=1\n",
        "p_intentvaln = []\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentvaln.append(i[:-1].lower())\n",
        "      c+=1\n",
        "with open('yv.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      #i = i.split(' ')\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippetval.append(i.lower())\n",
        "val_pairs = [[p_intentvaln[i],p_snippetval[i]] for i in iiv]\n",
        "p_intenttest = []\n",
        "p_snippettest = []\n",
        "ii = []\n",
        "c= 0\n",
        "with open('xt.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest:\n",
        "        ii.append(c)\n",
        "      c+=1\n",
        "      p_intenttest.append(i[:-1].lower())\n",
        "with open('yt.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippettest.append(i.lower())\n",
        "test_pairs = [[p_intenttest[i],p_snippettest[i]] for i in ii]\n",
        "p_intenttest1 = []\n",
        "p_snippettest1 = []\n",
        "with open('xt1.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest1.append(i[:-1].lower())\n",
        "with open('yt1.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest1.append(i.lower())\n",
        "test_pairs1 = [[p_intenttest1[i],p_snippettest1[i]] for i in ii]\n",
        "p_intenttest2 = []\n",
        "p_snippettest2 = []\n",
        "with open('xt2.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest2.append(i[:-1].lower())\n",
        "with open('yt2.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest2.append(i.lower())\n",
        "test_pairs2 = [[p_intenttest2[i],p_snippettest2[i]] for i in ii]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_N-qCJC9iYA",
        "outputId": "6cf600b9-c487-4300-fad7-f911fdceb3ac"
      },
      "source": [
        "\n",
        "SRC_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                                tokenize=tokenizer,\n",
        "\n",
        "                                preprocessing=lambda x: ['sos'] + x + ['eos'],\n",
        "                                \n",
        "                                )\n",
        "TARG_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                                 tokenize=tokenizer,\n",
        "\n",
        "                                 preprocessing=lambda x: ['sos'] + x + ['eos'],\n",
        "                                 )\n",
        "                                 \n",
        "\n",
        "\n",
        "def get_dataset(pairs, src, targ):\n",
        "    fields = [('src', src), ('targ', targ)]  \n",
        "    examples = []  \n",
        "    for src, targ in tqdm(pairs): \n",
        "        examples.append(torchtext.legacy.data.Example.fromlist([src, targ], fields))\n",
        "    return examples, fields\n",
        "\n",
        "\n",
        "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
        "\n",
        "ds_train = torchtext.legacy.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
        "ds_val = torchtext.legacy.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
        "ds_test = torchtext.legacy.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n",
        "\n",
        "ds_test1 = torchtext.legacy.data.Dataset(*get_dataset(test_pairs1, SRC_TEXT, TARG_TEXT))\n",
        "ds_test2 = torchtext.legacy.data.Dataset(*get_dataset(test_pairs2, SRC_TEXT, TARG_TEXT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51942/51942 [00:00<00:00, 72313.59it/s]\n",
            "100%|██████████| 12977/12977 [00:00<00:00, 107221.90it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 48842.71it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 110058.99it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 42511.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvDPrnG8-sok",
        "outputId": "3a56c99e-ae59-45e0-a625-e870def52ea4"
      },
      "source": [
        "SRC_TEXT.build_vocab(ds_train) \n",
        "print(len(SRC_TEXT.vocab))\n",
        "print(SRC_TEXT.vocab.itos[0])\n",
        "print(SRC_TEXT.vocab.itos[1])\n",
        "print(SRC_TEXT.vocab.itos[2])\n",
        "print(SRC_TEXT.vocab.itos[3])\n",
        "print(SRC_TEXT.vocab.stoi['sos'])\n",
        "print(SRC_TEXT.vocab.stoi['eos'])\n",
        "\n",
        "\n",
        "TARG_TEXT.build_vocab(ds_train)\n",
        "print(len(TARG_TEXT.vocab))\n",
        "print(TARG_TEXT.vocab.itos[0])\n",
        "print(TARG_TEXT.vocab.itos[1])\n",
        "print(TARG_TEXT.vocab.itos[2])\n",
        "print(TARG_TEXT.vocab.itos[3])\n",
        "print(TARG_TEXT.vocab.stoi['sos'])\n",
        "print(TARG_TEXT.vocab.stoi['eos'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "<unk>\n",
            "<pad>\n",
            " \n",
            "e\n",
            "14\n",
            "13\n",
            "62\n",
            "<unk>\n",
            "<pad>\n",
            " \n",
            ")\n",
            "15\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQrTIYg2FBK5"
      },
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, data_iter):\n",
        "        self.data_iter = data_iter\n",
        "        self.length = len(data_iter)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __iter__(self):\n",
        "        # switch to batch first\n",
        "        for batch in self.data_iter:\n",
        "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvNMPJJvTtB1"
      },
      "source": [
        "## Positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRSCw9-F0vm"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    # 2*(i//2) make sure that calculating 1/10000^(2i/d)\n",
        "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))\n",
        "    return pos * angle_rates  \n",
        "\n",
        "def positional_encoding(position, d_model):  # d_model = embedding_dim\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  \n",
        "                            np.arange(d_model)[np.newaxis, :], \n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]  \n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLYq74ykUwei"
      },
      "source": [
        "## Masking\n",
        "The padding mask indicates where is a padding token: it outputs a 1 at locations of padding tokens, and a 0 otherwise. It guarantee that the model does not treat padding as the input.\n",
        "\n",
        "The look-ahead mask is used to mask the future tokens in a target sequence. In other words, the mask indicates which target tokens should not be used. For example, to predict the third target token, only the first and second target token will be allowed to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca2NxbI-FKsQ"
      },
      "source": [
        "pad = 1 # When building vocab in Pytorch, index 1 represent padding token \n",
        "def create_padding_mask(seq):  # seq [b, seq_len]\n",
        "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
        "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
        "def create_look_ahead_mask(size):  # seq_len\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
        "    return mask  # [seq_len, seq_len]\n",
        "def create_mask(inp, targ):\n",
        "    # encoder padding mask\n",
        "    # Used in the self-attention in the encoder.\n",
        "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len]\n",
        "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
        "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # combine 2 masks =>[b,1,targ_seq_len,targ_seq_len]\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7P-3DVrVDWh"
      },
      "source": [
        "## Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9TYOdDFUo4"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  #[..., seq_len_q, seq_len_k]\n",
        "    # scale matmul_qk\n",
        "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # depth of k\n",
        "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:  # mask: [batch_size, 1, 1, seq_len]\n",
        "        # The mask is multiplied with -1e9 (close to negative infinity). \n",
        "        # This is done because the mask is summed with the scaled matrix \n",
        "        # multiplication of Q and K and is applied \n",
        "        # immediately before a softmax.\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  \n",
        "\n",
        "    output = torch.matmul(attention_weights, v)  \n",
        "    return output, attention_weights  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT3hi4AmFb2F"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0  #need to make sure each head have same dimension\n",
        "\n",
        "        self.depth = d_model // self.num_heads \n",
        "\n",
        "        self.wq = torch.nn.Linear(d_model, d_model)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads,\n",
        "                   self.depth)  # [b, seq_len, d_model]=>[b, seq_len, num_head, depth]\n",
        "        return x.transpose(1, 2)  # [b, seq_len, num_head, depth]=>[b, num_head, seq_len, depth]\n",
        "\n",
        "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim = d_model\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.wq(q)  # =>[batch_size, seq_len, d_model]\n",
        "        k = self.wk(k)  # =>[batch_size, seq_len, d_model]\n",
        "        v = self.wq(v)  # =>[batch_sizeb, seq_len, d_model]\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        k = self.split_heads(k, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        v = self.split_heads(v, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # => [batch_size, num_head, seq_len_q, depth], [batch_size, num_head, seq_len_q, seq_len_k]\n",
        "\n",
        "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[batch_size, seq_len_q, num_head, depth]\n",
        "        concatenate_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[batch_size, seq_len_q, d_model]\n",
        "\n",
        "        output = self.final_linear(concatenate_attention)  # =>[batch_size, seq_len_q, d_model]\n",
        "        return output, attention_weights  # [batch_size, seq_len_q, d_model], [batch_size, num_head, seq_len_q, seq_len_k]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKDxQ5aNWNyW"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdIytQNfOJlW"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    feed_forward_net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(d_model, dff),  # [batch_size, seq_len, d_model]=>[batch_size, seq_len, dff]\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(dff, d_model),  # [batch_size, seq_len, dff=2048]=>[batch_size, seq_len, d_model]\n",
        "    )\n",
        "    return feed_forward_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZCF3FNlWdvC"
      },
      "source": [
        "## Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hjuEDQ4ONcA"
      },
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)  # MultiHeadAttention（padding mask）(self-attention)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, inp_seq_len, embedding_dim] embedding_dim =d_model\n",
        "    # mask [batch_size,1,1,inp_seq_len]\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # =>[batch_size, seq_len, d_model]\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # =>[batch_size, seq_len, d_model]\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  #  =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        return out2  # [batch_size, seq_len, d_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x6hgOIjFepX"
      },
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads)  # MultiHeadAttention（look ahead mask and padding mask）(self-attention)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # MultiHeadAttention（padding mask）(encoder-decoder attention)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "        self.dropout3 = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, targ_seq_len, embedding_dim] embedding_dim =d_model\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len] \n",
        "    # enc_output [batch_size, inp_seq_len, d_model]\n",
        "    # padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
        "                                               look_ahead_mask)  # =>[batch_size, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)  #  [batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
        "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
        "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
        "                                               padding_mask)  # =>[batch_size, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(out1 + attn2)  #  [batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # =>[batch_size, targ_seq_len, d_model]\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)  # =>[batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "        # [batch_size, targ_seq_len, d_model], [batch_size, num_heads, targ_seq_len, targ_seq_len], [batch_size, num_heads, targ_seq_len, inp_seq_len]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6MLlHSFoIt"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  \n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 pre_train,\n",
        "                 input_vocab_size, \n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, inp_seq_len]\n",
        "    # mask [batch_size, 1, 1, inp_sel_len]\n",
        "    def forward(self, x, mask):\n",
        "        inp_seq_len = x.shape[-1]\n",
        "\n",
        "        # adding embedding and position encoding\n",
        "        x = self.embedding(x)  # [batch_size, inp_seq_len]=>[batch_size, inp_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
        "        pos_encoding = pos_encoding.cuda()  \n",
        "        x += pos_encoding  # [batch_size, inp_seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)  # [batch_size, inp_seq_len, d_model]=>[batch_size, inp_seq_len, d_model]\n",
        "        return x  # [batch_size, inp_seq_len, d_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlgfWG9pORK_"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  # N个encoder layer\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  # dimension of ffn\n",
        "                 target_vocab_size,  # target vocab size\n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
        "        self.pos_encoding = positional_encoding(1000,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "\n",
        "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
        "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [b, targ_seq_len]\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len] \n",
        "    # enc_output [batch_size, inp_seq_len, d_model]\n",
        "    # padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        targ_seq_len = x.shape[-1]\n",
        "\n",
        "        attention_weights = {}\n",
        "\n",
        "        # adding embedding and position encoding\n",
        "        x = self.embedding(x)  # [batch_size, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
        "        pos_encoding = pos_encoding.cuda() #\n",
        "        x += pos_encoding  # [batch_size, inp_seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "            # => [batch_size, targ_seq_len, d_model], [batch_size, num_heads, targ_seq_len, targ_seq_len], [batch_size, num_heads, targ_seq_len, inp_seq_len]\n",
        "\n",
        "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
        "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
        "\n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HIqB-SBkEI"
      },
      "source": [
        "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "# real [batch_size, targ_seq_len]\n",
        "# pred [batch_size, targ_seq_len, target_vocab_size]\n",
        "def mask_loss_func(real, pred):\n",
        "    # print(real.shape, pred.shape)\n",
        "    # _loss = loss_object(pred, real) # [batch_size, targ_seq_len]\n",
        "    _loss = loss_object(pred.transpose(-1, -2), real)  # [batch_size, targ_seq_len]\n",
        "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [batch_size, targ_seq_len] \n",
        "    _loss *= mask\n",
        "    return _loss.sum() / mask.sum().item()\n",
        "def mask_accuracy_func(real, pred):\n",
        "    _pred = pred.argmax(dim=-1)  # [batch_size, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
        "    corrects = _pred.eq(real)  # [batch_size, targ_seq_len] bool value\n",
        "    mask = torch.logical_not(real.eq(pad))  # [batch_size, targ_seq_len] where index != 1 \n",
        "    corrects *= mask\n",
        "    return corrects.sum().float() / mask.sum().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q35PXjOZ4Ir"
      },
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  # N个encoder layer\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 input_vocab_size,  \n",
        "                 target_vocab_size,  \n",
        "                 pre_train,\n",
        "                 pe_input,  # input max_pos_encoding\n",
        "                 pe_target,\n",
        "                 # input max_pos_encoding\n",
        "                 rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,pre_train,\n",
        "                               input_vocab_size,\n",
        "                               pe_input,\n",
        "                               rate)\n",
        "        self.decoder = Decoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,\n",
        "                               target_vocab_size,\n",
        "                               pe_target,\n",
        "                               rate)\n",
        "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    # inp [batch_size, inp_seq_len]\n",
        "    # targ [batch_size, targ_seq_len]\n",
        "    # enc_padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len]\n",
        "    # dec_padding_mask [batch_size, 1, 1, inp_seq_len] #\n",
        "    def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[batch_size, inp_seq_len, d_model]\n",
        "        dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        # => [batch_size, targ_seq_len, d_model],\n",
        "        # {'..block1': [batch_size, num_heads, targ_seq_len, targ_seq_len],\n",
        "        #  '..block2': [batch_size, num_heads, targ_seq_len, inp_seq_len], ...}\n",
        "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFbK6wvlbPRW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HVZxgUzB_Yg"
      },
      "source": [
        "def train_step(model, inp, targ):\n",
        "    # tar_inp as input to decoder due to teacher force\n",
        "    # tar_real is shift 1 prosition of in postion of tar_inp，tar_real include the groudtruth of next predicted token\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
        "\n",
        "    inp = inp.to(device)\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "    # print('device:', inp.device, targ_inp)\n",
        "\n",
        "    model.train()  # set train mode\n",
        "\n",
        "    optimizer.zero_grad()  \n",
        "\n",
        "    # forward\n",
        "    prediction, _ = model(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "    loss = mask_loss_func(targ_real, prediction)\n",
        "    metric = mask_accuracy_func(targ_real, prediction)\n",
        "\n",
        "    # backward\n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "\n",
        "    return loss.item(), metric.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bPHR9L2CJ6o"
      },
      "source": [
        "def validate_step(model, inp, targ):\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
        "\n",
        "    inp = inp.to(device)\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "\n",
        "    model.eval()  # set eval mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # forward\n",
        "        prediction, _ = model(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "        val_loss = mask_loss_func(targ_real, prediction)\n",
        "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
        "\n",
        "    return val_loss.item(), val_metric.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTpEoM6HCaoc"
      },
      "source": [
        "EPOCHS = 20 # 50 # 30  # 20\n",
        "\n",
        "print_trainstep_every = 50  # 每50个step做一次打印\n",
        "\n",
        "metric_name = 'acc'\n",
        "\n",
        "def printbar():\n",
        "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
        "    print('\\n' + \"==========\"*8 + '%s'%nowtime)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLUZb_aCjDl"
      },
      "source": [
        "# training function\n",
        "def train_model(model, epochs,ds_train, ds_val ,ds_test,ds_test1,ds_test2, print_every,BATCH_SIZE):\n",
        "    starttime = time.time()\n",
        "    print('*' * 27, 'start training...')\n",
        "    printbar()\n",
        "\n",
        "    best_acc = 0.\n",
        "    # initialize record frame work\n",
        "    df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name,'test_loss', 'test_' + metric_name, 'test1_loss', 'test1_' + metric_name,'test2_loss', 'test2_' + metric_name])\n",
        "    for epoch in range(1,epochs+1):\n",
        "        # laod data\n",
        "        train_iter, val_iter,test_iter,test_iter1,test_iter2 = torchtext.legacy.data.Iterator.splits(\n",
        "    (ds_train, ds_val,ds_test,ds_test1,ds_test2),\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    batch_sizes=(BATCH_SIZE, 256,256,256,256),\n",
        "    shuffle = True\n",
        ")\n",
        "        val_dataloader = DataLoader(val_iter)\n",
        "        test_dataloader = DataLoader(test_iter) \n",
        "        test_dataloader1 = DataLoader(test_iter1)\n",
        "        test_dataloader2 = DataLoader(test_iter2)\n",
        "        train_dataloader = DataLoader(train_iter)\n",
        "        loss_sum = 0.\n",
        "        metric_sum = 0.\n",
        "\n",
        "        for step, (inp, targ) in enumerate(train_dataloader,start = 1):\n",
        "            # inp [64, 10] , targ [64, 10]\n",
        "            loss, metric = train_step(model, inp, targ)\n",
        "\n",
        "            loss_sum += loss\n",
        "            metric_sum += metric\n",
        "            if step % print_every == 0:\n",
        "                print('*' * 8, f'[step = {step}] loss: {loss_sum / step:.3f}, {metric_name}: {metric_sum / step:.3f}')\n",
        "\n",
        "        # adjust lr\n",
        "        scheduler.step()\n",
        "        val_loss_sum = 0.\n",
        "        val_metric_sum = 0.\n",
        "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
        "\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            val_loss_sum += loss\n",
        "            val_metric_sum += metric\n",
        "\n",
        "        test_loss_sum = 0.\n",
        "        test_metric_sum = 0.\n",
        "        for test_step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
        "\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "            test_loss_sum += loss\n",
        "            test_metric_sum += metric\n",
        "        test_loss_sum1 = 0.\n",
        "        test_metric_sum1 = 0.\n",
        "        for test_step1, (inp, targ) in enumerate(test_dataloader1, start=1):\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            test_loss_sum1 += loss\n",
        "            test_metric_sum1 += metric\n",
        "        test_loss_sum2 = 0.\n",
        "        test_metric_sum2 = 0.\n",
        "        bleu = []\n",
        "        for test_step2, (inp, targ) in enumerate(test_dataloader2, start=1):\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            test_loss_sum2 += loss\n",
        "            test_metric_sum2 += metric\n",
        "        if epoch<1000:\n",
        "          record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step,\n",
        "                  test_loss_sum/test_step, test_metric_sum/test_step,test_loss_sum1/test_step1, test_metric_sum1/test_step1,test_loss_sum2/test_step2, test_metric_sum2/test_step2)\n",
        "          df_history.loc[epoch - 1] = record\n",
        "          print('EPOCH = {} loss: {:.3f}, {}: {:.3f}, val_loss: {:.3f}, val_{}: {:.3f},test_loss: {:.3f}, test_{}: {:.3f}, test1_loss: {:.3f}, test1_{}: {:.3f},test2_loss: {:.3f}, test2_{}: {:.3f}'.format(\n",
        "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4],record[5],metric_name, record[6],record[7],metric_name,record[8],record[9],metric_name,record[10]))\n",
        "          printbar()\n",
        "          # early stop\n",
        "          if len(df_history)>=11:\n",
        "            if record[3]>=df_history.loc[epoch - 11][3]:\n",
        "              break\n",
        "          else:\n",
        "            continue\n",
        "    print('finishing training...')\n",
        "    endtime = time.time()\n",
        "    time_elapsed = endtime - starttime\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    return df_history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POIzwADYcQX9"
      },
      "source": [
        "## Generating FOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zf6tNjsG5Ew"
      },
      "source": [
        "def tokenizer_encode(tokenize, sentence, vocab):\n",
        "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
        "    # print(len(vocab))\n",
        "    sentence =sentence.lower()\n",
        "    # print(type(sentence)) # str\n",
        "    sentence = tokenize(sentence)  # list\n",
        "    sentence = ['sos'] + sentence + ['eos']\n",
        "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
        "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
        "    return sentence_ids\n",
        "\n",
        "\n",
        "\n",
        "def tokenzier_decode1(sentence_ids, vocab):\n",
        "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
        "    # print(sentence)\n",
        "    return \" \".join(sentence[1:])\n",
        "\n",
        "def tokenzier_decode_char(sentence_ids, vocab):\n",
        "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
        "    # print(sentence)\n",
        "    return \"\".join(sentence[1:])\n",
        "\n",
        "def evaluate(model, inp_sentence):\n",
        "    model.eval()  # 设置eval mode\n",
        "\n",
        "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
        "    # print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
        "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[batch_size=1, inp_seq_len=10]\n",
        "    # print(encoder_input.shape)\n",
        "    decoder_input = [TARG_TEXT.vocab.stoi['sos']]\n",
        "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[batch_size=1,seq_len=1]\n",
        "    # print(decoder_input.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu()) ################\n",
        "        encoder_input = encoder_input.to(device)\n",
        "        enc_padding_mask = enc_padding_mask.to(device)\n",
        "        enout = model.encoder(encoder_input, enc_padding_mask)\n",
        "        for i in range(200 + 2):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu()) ################\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            combined_mask = combined_mask.to(device)\n",
        "            dec_padding_mask = dec_padding_mask.to(device)\n",
        "            predictions, attention_weights = model.decoder(decoder_input, enout, combined_mask, dec_padding_mask)\n",
        "            predictions = model.final_layer(predictions)\n",
        "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
        "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
        "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['eos']:\n",
        "                return decoder_input.squeeze(dim=0), attention_weights\n",
        "\n",
        "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
        "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
        "            # decoder_input being longer\n",
        "    return decoder_input.squeeze(dim=0), attention_weights\n",
        "    # [targ_seq_len],\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnFzAdyOCt7f",
        "outputId": "edc840da-45c5-4404-a24f-49718dc3f6d7"
      },
      "source": [
        "num_layers = 3\n",
        "d_model =300\n",
        "dff = 600\n",
        "num_heads = 5\n",
        "\n",
        "input_vocab_size = len(SRC_TEXT.vocab) # 3901\n",
        "target_vocab_size = len(TARG_TEXT.vocab) # 2591\n",
        "dropout_rate = 0.3\n",
        "torch.cuda.empty_cache()\n",
        "transformer = Transformer(num_layers,\n",
        "                          d_model,\n",
        "                          num_heads,\n",
        "                          dff,\n",
        "                          input_vocab_size,\n",
        "                          target_vocab_size,\n",
        "                          None,\n",
        "                          pe_input=1000,#set 1000 if char-level\n",
        "                          pe_target=1000,#set 1000 if char-level\n",
        "                          \n",
        "                          rate=dropout_rate)\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(),lr=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer , 5,\n",
        "                gamma =0.9)\n",
        "df_history = train_model(transformer, 150, ds_train, ds_val, ds_test,ds_test1,ds_test2,print_trainstep_every,256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************** start training...\n",
            "\n",
            "================================================================================2021-09_27 08:11:25\n",
            "******** [step = 50] loss: 2.001, acc: 0.454\n",
            "******** [step = 100] loss: 1.747, acc: 0.508\n",
            "******** [step = 150] loss: 1.596, acc: 0.542\n",
            "******** [step = 200] loss: 1.489, acc: 0.565\n",
            "EPOCH = 1 loss: 1.484, acc: 0.566, val_loss: 1.073, val_acc: 0.660,test_loss: 1.079, test_acc: 0.658, test1_loss: 1.106, test1_acc: 0.653,test2_loss: 1.130, test2_acc: 0.648\n",
            "\n",
            "================================================================================2021-09_27 08:12:34\n",
            "******** [step = 50] loss: 1.106, acc: 0.652\n",
            "******** [step = 100] loss: 1.075, acc: 0.660\n",
            "******** [step = 150] loss: 1.052, acc: 0.666\n",
            "******** [step = 200] loss: 1.032, acc: 0.671\n",
            "EPOCH = 2 loss: 1.031, acc: 0.672, val_loss: 0.899, val_acc: 0.708,test_loss: 0.906, test_acc: 0.707, test1_loss: 0.945, test1_acc: 0.699,test2_loss: 0.980, test2_acc: 0.692\n",
            "\n",
            "================================================================================2021-09_27 08:13:42\n",
            "******** [step = 50] loss: 0.947, acc: 0.694\n",
            "******** [step = 100] loss: 0.933, acc: 0.698\n",
            "******** [step = 150] loss: 0.922, acc: 0.701\n",
            "******** [step = 200] loss: 0.911, acc: 0.705\n",
            "EPOCH = 3 loss: 0.910, acc: 0.705, val_loss: 0.815, val_acc: 0.732,test_loss: 0.823, test_acc: 0.730, test1_loss: 0.870, test1_acc: 0.720,test2_loss: 0.912, test2_acc: 0.711\n",
            "\n",
            "================================================================================2021-09_27 08:14:51\n",
            "******** [step = 50] loss: 0.863, acc: 0.719\n",
            "******** [step = 100] loss: 0.855, acc: 0.722\n",
            "******** [step = 150] loss: 0.850, acc: 0.723\n",
            "******** [step = 200] loss: 0.847, acc: 0.724\n",
            "EPOCH = 4 loss: 0.847, acc: 0.724, val_loss: 0.762, val_acc: 0.751,test_loss: 0.769, test_acc: 0.748, test1_loss: 0.822, test1_acc: 0.738,test2_loss: 0.869, test2_acc: 0.728\n",
            "\n",
            "================================================================================2021-09_27 08:15:59\n",
            "******** [step = 50] loss: 0.809, acc: 0.735\n",
            "******** [step = 100] loss: 0.805, acc: 0.737\n",
            "******** [step = 150] loss: 0.799, acc: 0.739\n",
            "******** [step = 200] loss: 0.793, acc: 0.741\n",
            "EPOCH = 5 loss: 0.793, acc: 0.741, val_loss: 0.716, val_acc: 0.765,test_loss: 0.723, test_acc: 0.763, test1_loss: 0.782, test1_acc: 0.750,test2_loss: 0.836, test2_acc: 0.739\n",
            "\n",
            "================================================================================2021-09_27 08:17:07\n",
            "******** [step = 50] loss: 0.765, acc: 0.749\n",
            "******** [step = 100] loss: 0.769, acc: 0.748\n",
            "******** [step = 150] loss: 0.762, acc: 0.750\n",
            "******** [step = 200] loss: 0.756, acc: 0.752\n",
            "EPOCH = 6 loss: 0.755, acc: 0.752, val_loss: 0.675, val_acc: 0.778,test_loss: 0.682, test_acc: 0.775, test1_loss: 0.749, test1_acc: 0.761,test2_loss: 0.810, test2_acc: 0.746\n",
            "\n",
            "================================================================================2021-09_27 08:18:16\n",
            "******** [step = 50] loss: 0.735, acc: 0.759\n",
            "******** [step = 100] loss: 0.731, acc: 0.760\n",
            "******** [step = 150] loss: 0.729, acc: 0.761\n",
            "******** [step = 200] loss: 0.725, acc: 0.762\n",
            "EPOCH = 7 loss: 0.725, acc: 0.762, val_loss: 0.648, val_acc: 0.786,test_loss: 0.656, test_acc: 0.783, test1_loss: 0.727, test1_acc: 0.767,test2_loss: 0.792, test2_acc: 0.753\n",
            "\n",
            "================================================================================2021-09_27 08:19:25\n",
            "******** [step = 50] loss: 0.706, acc: 0.768\n",
            "******** [step = 100] loss: 0.703, acc: 0.769\n",
            "******** [step = 150] loss: 0.700, acc: 0.770\n",
            "******** [step = 200] loss: 0.697, acc: 0.771\n",
            "EPOCH = 8 loss: 0.697, acc: 0.771, val_loss: 0.633, val_acc: 0.791,test_loss: 0.640, test_acc: 0.789, test1_loss: 0.718, test1_acc: 0.772,test2_loss: 0.788, test2_acc: 0.757\n",
            "\n",
            "================================================================================2021-09_27 08:20:33\n",
            "******** [step = 50] loss: 0.710, acc: 0.768\n",
            "******** [step = 100] loss: 0.709, acc: 0.767\n",
            "******** [step = 150] loss: 0.698, acc: 0.770\n",
            "******** [step = 200] loss: 0.690, acc: 0.773\n",
            "EPOCH = 9 loss: 0.690, acc: 0.773, val_loss: 0.613, val_acc: 0.797,test_loss: 0.620, test_acc: 0.795, test1_loss: 0.701, test1_acc: 0.777,test2_loss: 0.776, test2_acc: 0.761\n",
            "\n",
            "================================================================================2021-09_27 08:21:42\n",
            "******** [step = 50] loss: 0.664, acc: 0.781\n",
            "******** [step = 100] loss: 0.662, acc: 0.781\n",
            "******** [step = 150] loss: 0.659, acc: 0.782\n",
            "******** [step = 200] loss: 0.656, acc: 0.783\n",
            "EPOCH = 10 loss: 0.656, acc: 0.783, val_loss: 0.590, val_acc: 0.804,test_loss: 0.598, test_acc: 0.802, test1_loss: 0.684, test1_acc: 0.783,test2_loss: 0.762, test2_acc: 0.766\n",
            "\n",
            "================================================================================2021-09_27 08:22:50\n",
            "******** [step = 50] loss: 0.645, acc: 0.787\n",
            "******** [step = 100] loss: 0.643, acc: 0.788\n",
            "******** [step = 150] loss: 0.643, acc: 0.788\n",
            "******** [step = 200] loss: 0.641, acc: 0.788\n",
            "EPOCH = 11 loss: 0.641, acc: 0.788, val_loss: 0.571, val_acc: 0.811,test_loss: 0.579, test_acc: 0.809, test1_loss: 0.669, test1_acc: 0.790,test2_loss: 0.750, test2_acc: 0.771\n",
            "\n",
            "================================================================================2021-09_27 08:23:59\n",
            "******** [step = 50] loss: 0.631, acc: 0.792\n",
            "******** [step = 100] loss: 0.628, acc: 0.792\n",
            "******** [step = 150] loss: 0.626, acc: 0.793\n",
            "******** [step = 200] loss: 0.623, acc: 0.794\n",
            "EPOCH = 12 loss: 0.623, acc: 0.794, val_loss: 0.568, val_acc: 0.812,test_loss: 0.576, test_acc: 0.810, test1_loss: 0.671, test1_acc: 0.789,test2_loss: 0.757, test2_acc: 0.769\n",
            "\n",
            "================================================================================2021-09_27 08:25:07\n",
            "******** [step = 50] loss: 0.616, acc: 0.796\n",
            "******** [step = 100] loss: 0.614, acc: 0.796\n",
            "******** [step = 150] loss: 0.613, acc: 0.797\n",
            "******** [step = 200] loss: 0.611, acc: 0.798\n",
            "EPOCH = 13 loss: 0.611, acc: 0.798, val_loss: 0.556, val_acc: 0.816,test_loss: 0.564, test_acc: 0.813, test1_loss: 0.662, test1_acc: 0.792,test2_loss: 0.751, test2_acc: 0.772\n",
            "\n",
            "================================================================================2021-09_27 08:26:15\n",
            "******** [step = 50] loss: 0.607, acc: 0.799\n",
            "******** [step = 100] loss: 0.605, acc: 0.800\n",
            "******** [step = 150] loss: 0.601, acc: 0.801\n",
            "******** [step = 200] loss: 0.599, acc: 0.801\n",
            "EPOCH = 14 loss: 0.599, acc: 0.801, val_loss: 0.545, val_acc: 0.819,test_loss: 0.553, test_acc: 0.816, test1_loss: 0.657, test1_acc: 0.795,test2_loss: 0.751, test2_acc: 0.774\n",
            "\n",
            "================================================================================2021-09_27 08:27:24\n",
            "******** [step = 50] loss: 0.593, acc: 0.803\n",
            "******** [step = 100] loss: 0.591, acc: 0.804\n",
            "******** [step = 150] loss: 0.591, acc: 0.804\n",
            "******** [step = 200] loss: 0.587, acc: 0.805\n",
            "EPOCH = 15 loss: 0.587, acc: 0.805, val_loss: 0.530, val_acc: 0.823,test_loss: 0.538, test_acc: 0.821, test1_loss: 0.648, test1_acc: 0.798,test2_loss: 0.749, test2_acc: 0.776\n",
            "\n",
            "================================================================================2021-09_27 08:28:32\n",
            "******** [step = 50] loss: 0.578, acc: 0.808\n",
            "******** [step = 100] loss: 0.577, acc: 0.808\n",
            "******** [step = 150] loss: 0.574, acc: 0.809\n",
            "******** [step = 200] loss: 0.572, acc: 0.809\n",
            "EPOCH = 16 loss: 0.572, acc: 0.809, val_loss: 0.515, val_acc: 0.829,test_loss: 0.522, test_acc: 0.827, test1_loss: 0.632, test1_acc: 0.804,test2_loss: 0.732, test2_acc: 0.782\n",
            "\n",
            "================================================================================2021-09_27 08:29:41\n",
            "******** [step = 50] loss: 0.575, acc: 0.808\n",
            "******** [step = 100] loss: 0.570, acc: 0.810\n",
            "******** [step = 150] loss: 0.567, acc: 0.811\n",
            "******** [step = 200] loss: 0.564, acc: 0.812\n",
            "EPOCH = 17 loss: 0.564, acc: 0.812, val_loss: 0.514, val_acc: 0.829,test_loss: 0.522, test_acc: 0.827, test1_loss: 0.637, test1_acc: 0.803,test2_loss: 0.743, test2_acc: 0.781\n",
            "\n",
            "================================================================================2021-09_27 08:30:49\n",
            "******** [step = 50] loss: 0.557, acc: 0.815\n",
            "******** [step = 100] loss: 0.557, acc: 0.814\n",
            "******** [step = 150] loss: 0.555, acc: 0.815\n",
            "******** [step = 200] loss: 0.553, acc: 0.815\n",
            "EPOCH = 18 loss: 0.553, acc: 0.815, val_loss: 0.514, val_acc: 0.829,test_loss: 0.522, test_acc: 0.827, test1_loss: 0.634, test1_acc: 0.804,test2_loss: 0.737, test2_acc: 0.783\n",
            "\n",
            "================================================================================2021-09_27 08:31:58\n",
            "******** [step = 50] loss: 0.558, acc: 0.814\n",
            "******** [step = 100] loss: 0.552, acc: 0.815\n",
            "******** [step = 150] loss: 0.549, acc: 0.816\n",
            "******** [step = 200] loss: 0.546, acc: 0.817\n",
            "EPOCH = 19 loss: 0.546, acc: 0.817, val_loss: 0.504, val_acc: 0.832,test_loss: 0.510, test_acc: 0.831, test1_loss: 0.630, test1_acc: 0.806,test2_loss: 0.740, test2_acc: 0.783\n",
            "\n",
            "================================================================================2021-09_27 08:33:06\n",
            "******** [step = 50] loss: 0.540, acc: 0.819\n",
            "******** [step = 100] loss: 0.539, acc: 0.819\n",
            "******** [step = 150] loss: 0.541, acc: 0.819\n",
            "******** [step = 200] loss: 0.539, acc: 0.820\n",
            "EPOCH = 20 loss: 0.539, acc: 0.820, val_loss: 0.490, val_acc: 0.837,test_loss: 0.496, test_acc: 0.834, test1_loss: 0.616, test1_acc: 0.810,test2_loss: 0.726, test2_acc: 0.787\n",
            "\n",
            "================================================================================2021-09_27 08:34:15\n",
            "******** [step = 50] loss: 0.531, acc: 0.823\n",
            "******** [step = 100] loss: 0.530, acc: 0.823\n",
            "******** [step = 150] loss: 0.529, acc: 0.823\n",
            "******** [step = 200] loss: 0.527, acc: 0.824\n",
            "EPOCH = 21 loss: 0.526, acc: 0.824, val_loss: 0.485, val_acc: 0.838,test_loss: 0.491, test_acc: 0.836, test1_loss: 0.617, test1_acc: 0.811,test2_loss: 0.732, test2_acc: 0.787\n",
            "\n",
            "================================================================================2021-09_27 08:35:23\n",
            "******** [step = 50] loss: 0.523, acc: 0.825\n",
            "******** [step = 100] loss: 0.522, acc: 0.825\n",
            "******** [step = 150] loss: 0.521, acc: 0.825\n",
            "******** [step = 200] loss: 0.519, acc: 0.826\n",
            "EPOCH = 22 loss: 0.519, acc: 0.826, val_loss: 0.481, val_acc: 0.840,test_loss: 0.487, test_acc: 0.838, test1_loss: 0.613, test1_acc: 0.813,test2_loss: 0.728, test2_acc: 0.788\n",
            "\n",
            "================================================================================2021-09_27 08:36:31\n",
            "******** [step = 50] loss: 0.517, acc: 0.827\n",
            "******** [step = 100] loss: 0.516, acc: 0.827\n",
            "******** [step = 150] loss: 0.515, acc: 0.827\n",
            "******** [step = 200] loss: 0.513, acc: 0.828\n",
            "EPOCH = 23 loss: 0.513, acc: 0.828, val_loss: 0.468, val_acc: 0.844,test_loss: 0.475, test_acc: 0.841, test1_loss: 0.601, test1_acc: 0.816,test2_loss: 0.717, test2_acc: 0.792\n",
            "\n",
            "================================================================================2021-09_27 08:37:40\n",
            "******** [step = 50] loss: 0.511, acc: 0.829\n",
            "******** [step = 100] loss: 0.510, acc: 0.829\n",
            "******** [step = 150] loss: 0.508, acc: 0.829\n",
            "******** [step = 200] loss: 0.507, acc: 0.829\n",
            "EPOCH = 24 loss: 0.507, acc: 0.829, val_loss: 0.472, val_acc: 0.842,test_loss: 0.478, test_acc: 0.840, test1_loss: 0.607, test1_acc: 0.814,test2_loss: 0.726, test2_acc: 0.789\n",
            "\n",
            "================================================================================2021-09_27 08:38:48\n",
            "******** [step = 50] loss: 0.528, acc: 0.824\n",
            "******** [step = 100] loss: 0.516, acc: 0.827\n",
            "******** [step = 150] loss: 0.511, acc: 0.829\n",
            "******** [step = 200] loss: 0.508, acc: 0.830\n",
            "EPOCH = 25 loss: 0.508, acc: 0.830, val_loss: 0.464, val_acc: 0.845,test_loss: 0.470, test_acc: 0.843, test1_loss: 0.599, test1_acc: 0.818,test2_loss: 0.717, test2_acc: 0.793\n",
            "\n",
            "================================================================================2021-09_27 08:39:57\n",
            "******** [step = 50] loss: 0.499, acc: 0.832\n",
            "******** [step = 100] loss: 0.497, acc: 0.833\n",
            "******** [step = 150] loss: 0.495, acc: 0.833\n",
            "******** [step = 200] loss: 0.494, acc: 0.834\n",
            "EPOCH = 26 loss: 0.494, acc: 0.834, val_loss: 0.460, val_acc: 0.846,test_loss: 0.465, test_acc: 0.844, test1_loss: 0.598, test1_acc: 0.818,test2_loss: 0.721, test2_acc: 0.793\n",
            "\n",
            "================================================================================2021-09_27 08:41:05\n",
            "******** [step = 50] loss: 0.492, acc: 0.835\n",
            "******** [step = 100] loss: 0.491, acc: 0.835\n",
            "******** [step = 150] loss: 0.490, acc: 0.835\n",
            "******** [step = 200] loss: 0.489, acc: 0.835\n",
            "EPOCH = 27 loss: 0.489, acc: 0.835, val_loss: 0.463, val_acc: 0.846,test_loss: 0.469, test_acc: 0.844, test1_loss: 0.603, test1_acc: 0.818,test2_loss: 0.726, test2_acc: 0.795\n",
            "\n",
            "================================================================================2021-09_27 08:42:14\n",
            "******** [step = 50] loss: 0.489, acc: 0.836\n",
            "******** [step = 100] loss: 0.488, acc: 0.836\n",
            "******** [step = 150] loss: 0.486, acc: 0.836\n",
            "******** [step = 200] loss: 0.485, acc: 0.837\n",
            "EPOCH = 28 loss: 0.485, acc: 0.837, val_loss: 0.450, val_acc: 0.850,test_loss: 0.456, test_acc: 0.848, test1_loss: 0.592, test1_acc: 0.821,test2_loss: 0.717, test2_acc: 0.796\n",
            "\n",
            "================================================================================2021-09_27 08:43:23\n",
            "******** [step = 50] loss: 0.482, acc: 0.837\n",
            "******** [step = 100] loss: 0.481, acc: 0.838\n",
            "******** [step = 150] loss: 0.480, acc: 0.838\n",
            "******** [step = 200] loss: 0.479, acc: 0.838\n",
            "EPOCH = 29 loss: 0.479, acc: 0.838, val_loss: 0.454, val_acc: 0.848,test_loss: 0.459, test_acc: 0.847, test1_loss: 0.602, test1_acc: 0.820,test2_loss: 0.733, test2_acc: 0.794\n",
            "\n",
            "================================================================================2021-09_27 08:44:31\n",
            "******** [step = 50] loss: 0.479, acc: 0.838\n",
            "******** [step = 100] loss: 0.478, acc: 0.839\n",
            "******** [step = 150] loss: 0.477, acc: 0.839\n",
            "******** [step = 200] loss: 0.475, acc: 0.840\n",
            "EPOCH = 30 loss: 0.475, acc: 0.840, val_loss: 0.452, val_acc: 0.849,test_loss: 0.456, test_acc: 0.848, test1_loss: 0.600, test1_acc: 0.820,test2_loss: 0.731, test2_acc: 0.795\n",
            "\n",
            "================================================================================2021-09_27 08:45:39\n",
            "******** [step = 50] loss: 0.471, acc: 0.841\n",
            "******** [step = 100] loss: 0.472, acc: 0.841\n",
            "******** [step = 150] loss: 0.470, acc: 0.841\n",
            "******** [step = 200] loss: 0.468, acc: 0.842\n",
            "EPOCH = 31 loss: 0.468, acc: 0.842, val_loss: 0.443, val_acc: 0.851,test_loss: 0.449, test_acc: 0.850, test1_loss: 0.598, test1_acc: 0.821,test2_loss: 0.734, test2_acc: 0.795\n",
            "\n",
            "================================================================================2021-09_27 08:46:48\n",
            "******** [step = 50] loss: 0.467, acc: 0.843\n",
            "******** [step = 100] loss: 0.467, acc: 0.842\n",
            "******** [step = 150] loss: 0.466, acc: 0.843\n",
            "******** [step = 200] loss: 0.464, acc: 0.843\n",
            "EPOCH = 32 loss: 0.464, acc: 0.843, val_loss: 0.438, val_acc: 0.854,test_loss: 0.444, test_acc: 0.852, test1_loss: 0.590, test1_acc: 0.825,test2_loss: 0.723, test2_acc: 0.798\n",
            "\n",
            "================================================================================2021-09_27 08:47:56\n",
            "******** [step = 50] loss: 0.464, acc: 0.843\n",
            "******** [step = 100] loss: 0.472, acc: 0.841\n",
            "******** [step = 150] loss: 0.475, acc: 0.840\n",
            "******** [step = 200] loss: 0.471, acc: 0.841\n",
            "EPOCH = 33 loss: 0.471, acc: 0.841, val_loss: 0.441, val_acc: 0.852,test_loss: 0.446, test_acc: 0.851, test1_loss: 0.594, test1_acc: 0.822,test2_loss: 0.730, test2_acc: 0.795\n",
            "\n",
            "================================================================================2021-09_27 08:49:05\n",
            "******** [step = 50] loss: 0.461, acc: 0.844\n",
            "******** [step = 100] loss: 0.461, acc: 0.844\n",
            "******** [step = 150] loss: 0.460, acc: 0.845\n",
            "******** [step = 200] loss: 0.458, acc: 0.845\n",
            "EPOCH = 34 loss: 0.458, acc: 0.845, val_loss: 0.431, val_acc: 0.856,test_loss: 0.436, test_acc: 0.854, test1_loss: 0.587, test1_acc: 0.825,test2_loss: 0.726, test2_acc: 0.797\n",
            "\n",
            "================================================================================2021-09_27 08:50:13\n",
            "******** [step = 50] loss: 0.458, acc: 0.845\n",
            "******** [step = 100] loss: 0.457, acc: 0.846\n",
            "******** [step = 150] loss: 0.456, acc: 0.846\n",
            "******** [step = 200] loss: 0.455, acc: 0.846\n",
            "EPOCH = 35 loss: 0.455, acc: 0.846, val_loss: 0.440, val_acc: 0.854,test_loss: 0.445, test_acc: 0.852, test1_loss: 0.597, test1_acc: 0.823,test2_loss: 0.737, test2_acc: 0.796\n",
            "\n",
            "================================================================================2021-09_27 08:51:22\n",
            "******** [step = 50] loss: 0.452, acc: 0.847\n",
            "******** [step = 100] loss: 0.451, acc: 0.847\n",
            "******** [step = 150] loss: 0.450, acc: 0.848\n",
            "******** [step = 200] loss: 0.449, acc: 0.848\n",
            "EPOCH = 36 loss: 0.449, acc: 0.848, val_loss: 0.433, val_acc: 0.855,test_loss: 0.438, test_acc: 0.854, test1_loss: 0.593, test1_acc: 0.824,test2_loss: 0.735, test2_acc: 0.797\n",
            "\n",
            "================================================================================2021-09_27 08:52:31\n",
            "******** [step = 50] loss: 0.449, acc: 0.848\n",
            "******** [step = 100] loss: 0.449, acc: 0.848\n",
            "******** [step = 150] loss: 0.448, acc: 0.848\n",
            "******** [step = 200] loss: 0.446, acc: 0.849\n",
            "EPOCH = 37 loss: 0.446, acc: 0.849, val_loss: 0.431, val_acc: 0.857,test_loss: 0.436, test_acc: 0.855, test1_loss: 0.592, test1_acc: 0.826,test2_loss: 0.735, test2_acc: 0.799\n",
            "\n",
            "================================================================================2021-09_27 08:53:39\n",
            "******** [step = 50] loss: 0.446, acc: 0.849\n",
            "******** [step = 100] loss: 0.446, acc: 0.849\n",
            "******** [step = 150] loss: 0.445, acc: 0.849\n",
            "******** [step = 200] loss: 0.444, acc: 0.850\n",
            "EPOCH = 38 loss: 0.444, acc: 0.849, val_loss: 0.438, val_acc: 0.855,test_loss: 0.443, test_acc: 0.853, test1_loss: 0.595, test1_acc: 0.824,test2_loss: 0.735, test2_acc: 0.798\n",
            "\n",
            "================================================================================2021-09_27 08:54:47\n",
            "******** [step = 50] loss: 0.443, acc: 0.850\n",
            "******** [step = 100] loss: 0.443, acc: 0.850\n",
            "******** [step = 150] loss: 0.442, acc: 0.850\n",
            "******** [step = 200] loss: 0.441, acc: 0.850\n",
            "EPOCH = 39 loss: 0.441, acc: 0.850, val_loss: 0.437, val_acc: 0.854,test_loss: 0.443, test_acc: 0.853, test1_loss: 0.602, test1_acc: 0.822,test2_loss: 0.750, test2_acc: 0.794\n",
            "\n",
            "================================================================================2021-09_27 08:55:56\n",
            "******** [step = 50] loss: 0.441, acc: 0.851\n",
            "******** [step = 100] loss: 0.440, acc: 0.851\n",
            "******** [step = 150] loss: 0.439, acc: 0.851\n",
            "******** [step = 200] loss: 0.438, acc: 0.851\n",
            "EPOCH = 40 loss: 0.438, acc: 0.851, val_loss: 0.433, val_acc: 0.856,test_loss: 0.438, test_acc: 0.854, test1_loss: 0.592, test1_acc: 0.825,test2_loss: 0.735, test2_acc: 0.798\n",
            "\n",
            "================================================================================2021-09_27 08:57:04\n",
            "******** [step = 50] loss: 0.437, acc: 0.852\n",
            "******** [step = 100] loss: 0.436, acc: 0.852\n",
            "******** [step = 150] loss: 0.435, acc: 0.852\n",
            "******** [step = 200] loss: 0.433, acc: 0.853\n",
            "EPOCH = 41 loss: 0.433, acc: 0.853, val_loss: 0.417, val_acc: 0.861,test_loss: 0.422, test_acc: 0.859, test1_loss: 0.580, test1_acc: 0.829,test2_loss: 0.726, test2_acc: 0.801\n",
            "\n",
            "================================================================================2021-09_27 08:58:13\n",
            "******** [step = 50] loss: 0.433, acc: 0.853\n",
            "******** [step = 100] loss: 0.432, acc: 0.853\n",
            "******** [step = 150] loss: 0.432, acc: 0.854\n",
            "******** [step = 200] loss: 0.430, acc: 0.854\n",
            "EPOCH = 42 loss: 0.430, acc: 0.854, val_loss: 0.420, val_acc: 0.860,test_loss: 0.425, test_acc: 0.858, test1_loss: 0.588, test1_acc: 0.828,test2_loss: 0.737, test2_acc: 0.800\n",
            "\n",
            "================================================================================2021-09_27 08:59:21\n",
            "******** [step = 50] loss: 0.441, acc: 0.851\n",
            "******** [step = 100] loss: 0.437, acc: 0.852\n",
            "******** [step = 150] loss: 0.433, acc: 0.853\n",
            "******** [step = 200] loss: 0.431, acc: 0.854\n",
            "EPOCH = 43 loss: 0.431, acc: 0.854, val_loss: 0.419, val_acc: 0.860,test_loss: 0.424, test_acc: 0.859, test1_loss: 0.588, test1_acc: 0.828,test2_loss: 0.739, test2_acc: 0.799\n",
            "\n",
            "================================================================================2021-09_27 09:00:30\n",
            "******** [step = 50] loss: 0.428, acc: 0.855\n",
            "******** [step = 100] loss: 0.427, acc: 0.855\n",
            "******** [step = 150] loss: 0.426, acc: 0.855\n",
            "******** [step = 200] loss: 0.425, acc: 0.855\n",
            "EPOCH = 44 loss: 0.425, acc: 0.855, val_loss: 0.414, val_acc: 0.861,test_loss: 0.418, test_acc: 0.860, test1_loss: 0.582, test1_acc: 0.829,test2_loss: 0.732, test2_acc: 0.800\n",
            "\n",
            "================================================================================2021-09_27 09:01:38\n",
            "******** [step = 50] loss: 0.427, acc: 0.855\n",
            "******** [step = 100] loss: 0.427, acc: 0.855\n",
            "******** [step = 150] loss: 0.424, acc: 0.856\n",
            "******** [step = 200] loss: 0.423, acc: 0.856\n",
            "EPOCH = 45 loss: 0.423, acc: 0.856, val_loss: 0.407, val_acc: 0.864,test_loss: 0.412, test_acc: 0.862, test1_loss: 0.573, test1_acc: 0.832,test2_loss: 0.721, test2_acc: 0.804\n",
            "\n",
            "================================================================================2021-09_27 09:02:47\n",
            "******** [step = 50] loss: 0.421, acc: 0.857\n",
            "******** [step = 100] loss: 0.421, acc: 0.857\n",
            "******** [step = 150] loss: 0.420, acc: 0.857\n",
            "******** [step = 200] loss: 0.419, acc: 0.857\n",
            "EPOCH = 46 loss: 0.419, acc: 0.857, val_loss: 0.406, val_acc: 0.864,test_loss: 0.411, test_acc: 0.863, test1_loss: 0.576, test1_acc: 0.832,test2_loss: 0.728, test2_acc: 0.804\n",
            "\n",
            "================================================================================2021-09_27 09:03:55\n",
            "******** [step = 50] loss: 0.419, acc: 0.857\n",
            "******** [step = 100] loss: 0.418, acc: 0.857\n",
            "******** [step = 150] loss: 0.418, acc: 0.858\n",
            "******** [step = 200] loss: 0.417, acc: 0.858\n",
            "EPOCH = 47 loss: 0.417, acc: 0.858, val_loss: 0.418, val_acc: 0.861,test_loss: 0.423, test_acc: 0.859, test1_loss: 0.590, test1_acc: 0.828,test2_loss: 0.746, test2_acc: 0.799\n",
            "\n",
            "================================================================================2021-09_27 09:05:03\n",
            "******** [step = 50] loss: 0.418, acc: 0.858\n",
            "******** [step = 100] loss: 0.417, acc: 0.858\n",
            "******** [step = 150] loss: 0.417, acc: 0.858\n",
            "******** [step = 200] loss: 0.417, acc: 0.858\n",
            "EPOCH = 48 loss: 0.417, acc: 0.858, val_loss: 0.404, val_acc: 0.865,test_loss: 0.408, test_acc: 0.863, test1_loss: 0.571, test1_acc: 0.833,test2_loss: 0.722, test2_acc: 0.804\n",
            "\n",
            "================================================================================2021-09_27 09:06:12\n",
            "******** [step = 50] loss: 0.414, acc: 0.859\n",
            "******** [step = 100] loss: 0.414, acc: 0.859\n",
            "******** [step = 150] loss: 0.413, acc: 0.859\n",
            "******** [step = 200] loss: 0.412, acc: 0.860\n",
            "EPOCH = 49 loss: 0.412, acc: 0.860, val_loss: 0.399, val_acc: 0.866,test_loss: 0.404, test_acc: 0.865, test1_loss: 0.573, test1_acc: 0.833,test2_loss: 0.730, test2_acc: 0.803\n",
            "\n",
            "================================================================================2021-09_27 09:07:20\n",
            "******** [step = 50] loss: 0.413, acc: 0.859\n",
            "******** [step = 100] loss: 0.413, acc: 0.859\n",
            "******** [step = 150] loss: 0.412, acc: 0.860\n",
            "******** [step = 200] loss: 0.411, acc: 0.860\n",
            "EPOCH = 50 loss: 0.411, acc: 0.860, val_loss: 0.400, val_acc: 0.866,test_loss: 0.405, test_acc: 0.865, test1_loss: 0.574, test1_acc: 0.833,test2_loss: 0.732, test2_acc: 0.804\n",
            "\n",
            "================================================================================2021-09_27 09:08:29\n",
            "******** [step = 50] loss: 0.410, acc: 0.860\n",
            "******** [step = 100] loss: 0.410, acc: 0.860\n",
            "******** [step = 150] loss: 0.408, acc: 0.861\n",
            "******** [step = 200] loss: 0.407, acc: 0.861\n",
            "EPOCH = 51 loss: 0.407, acc: 0.861, val_loss: 0.404, val_acc: 0.865,test_loss: 0.409, test_acc: 0.864, test1_loss: 0.582, test1_acc: 0.832,test2_loss: 0.743, test2_acc: 0.803\n",
            "\n",
            "================================================================================2021-09_27 09:09:37\n",
            "******** [step = 50] loss: 0.409, acc: 0.860\n",
            "******** [step = 100] loss: 0.408, acc: 0.861\n",
            "******** [step = 150] loss: 0.407, acc: 0.861\n",
            "******** [step = 200] loss: 0.406, acc: 0.862\n",
            "EPOCH = 52 loss: 0.406, acc: 0.862, val_loss: 0.401, val_acc: 0.866,test_loss: 0.406, test_acc: 0.865, test1_loss: 0.578, test1_acc: 0.834,test2_loss: 0.737, test2_acc: 0.805\n",
            "\n",
            "================================================================================2021-09_27 09:10:46\n",
            "******** [step = 50] loss: 0.405, acc: 0.862\n",
            "******** [step = 100] loss: 0.404, acc: 0.862\n",
            "******** [step = 150] loss: 0.404, acc: 0.862\n",
            "******** [step = 200] loss: 0.403, acc: 0.862\n",
            "EPOCH = 53 loss: 0.403, acc: 0.862, val_loss: 0.393, val_acc: 0.869,test_loss: 0.397, test_acc: 0.867, test1_loss: 0.570, test1_acc: 0.836,test2_loss: 0.730, test2_acc: 0.806\n",
            "\n",
            "================================================================================2021-09_27 09:11:54\n",
            "******** [step = 50] loss: 0.404, acc: 0.862\n",
            "******** [step = 100] loss: 0.404, acc: 0.862\n",
            "******** [step = 150] loss: 0.403, acc: 0.862\n",
            "******** [step = 200] loss: 0.402, acc: 0.863\n",
            "EPOCH = 54 loss: 0.402, acc: 0.863, val_loss: 0.392, val_acc: 0.869,test_loss: 0.397, test_acc: 0.868, test1_loss: 0.567, test1_acc: 0.836,test2_loss: 0.723, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:13:03\n",
            "******** [step = 50] loss: 0.405, acc: 0.862\n",
            "******** [step = 100] loss: 0.403, acc: 0.863\n",
            "******** [step = 150] loss: 0.401, acc: 0.863\n",
            "******** [step = 200] loss: 0.401, acc: 0.863\n",
            "EPOCH = 55 loss: 0.400, acc: 0.863, val_loss: 0.387, val_acc: 0.871,test_loss: 0.392, test_acc: 0.869, test1_loss: 0.567, test1_acc: 0.837,test2_loss: 0.729, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:14:11\n",
            "******** [step = 50] loss: 0.400, acc: 0.863\n",
            "******** [step = 100] loss: 0.400, acc: 0.863\n",
            "******** [step = 150] loss: 0.399, acc: 0.864\n",
            "******** [step = 200] loss: 0.398, acc: 0.864\n",
            "EPOCH = 56 loss: 0.398, acc: 0.864, val_loss: 0.392, val_acc: 0.869,test_loss: 0.396, test_acc: 0.868, test1_loss: 0.571, test1_acc: 0.836,test2_loss: 0.733, test2_acc: 0.806\n",
            "\n",
            "================================================================================2021-09_27 09:15:20\n",
            "******** [step = 50] loss: 0.398, acc: 0.864\n",
            "******** [step = 100] loss: 0.398, acc: 0.864\n",
            "******** [step = 150] loss: 0.397, acc: 0.865\n",
            "******** [step = 200] loss: 0.396, acc: 0.865\n",
            "EPOCH = 57 loss: 0.396, acc: 0.865, val_loss: 0.388, val_acc: 0.870,test_loss: 0.393, test_acc: 0.869, test1_loss: 0.568, test1_acc: 0.837,test2_loss: 0.730, test2_acc: 0.808\n",
            "\n",
            "================================================================================2021-09_27 09:16:28\n",
            "******** [step = 50] loss: 0.397, acc: 0.864\n",
            "******** [step = 100] loss: 0.397, acc: 0.864\n",
            "******** [step = 150] loss: 0.396, acc: 0.865\n",
            "******** [step = 200] loss: 0.395, acc: 0.865\n",
            "EPOCH = 58 loss: 0.395, acc: 0.865, val_loss: 0.386, val_acc: 0.871,test_loss: 0.391, test_acc: 0.869, test1_loss: 0.568, test1_acc: 0.838,test2_loss: 0.733, test2_acc: 0.808\n",
            "\n",
            "================================================================================2021-09_27 09:17:37\n",
            "******** [step = 50] loss: 0.396, acc: 0.865\n",
            "******** [step = 100] loss: 0.395, acc: 0.865\n",
            "******** [step = 150] loss: 0.399, acc: 0.864\n",
            "******** [step = 200] loss: 0.397, acc: 0.865\n",
            "EPOCH = 59 loss: 0.397, acc: 0.865, val_loss: 0.386, val_acc: 0.871,test_loss: 0.391, test_acc: 0.869, test1_loss: 0.568, test1_acc: 0.837,test2_loss: 0.732, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:18:45\n",
            "******** [step = 50] loss: 0.393, acc: 0.866\n",
            "******** [step = 100] loss: 0.393, acc: 0.866\n",
            "******** [step = 150] loss: 0.392, acc: 0.866\n",
            "******** [step = 200] loss: 0.391, acc: 0.866\n",
            "EPOCH = 60 loss: 0.391, acc: 0.866, val_loss: 0.386, val_acc: 0.871,test_loss: 0.390, test_acc: 0.870, test1_loss: 0.568, test1_acc: 0.837,test2_loss: 0.734, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:19:53\n",
            "******** [step = 50] loss: 0.389, acc: 0.867\n",
            "******** [step = 100] loss: 0.390, acc: 0.867\n",
            "******** [step = 150] loss: 0.389, acc: 0.867\n",
            "******** [step = 200] loss: 0.388, acc: 0.867\n",
            "EPOCH = 61 loss: 0.388, acc: 0.867, val_loss: 0.375, val_acc: 0.874,test_loss: 0.379, test_acc: 0.873, test1_loss: 0.557, test1_acc: 0.840,test2_loss: 0.721, test2_acc: 0.808\n",
            "\n",
            "================================================================================2021-09_27 09:21:02\n",
            "******** [step = 50] loss: 0.389, acc: 0.867\n",
            "******** [step = 100] loss: 0.389, acc: 0.867\n",
            "******** [step = 150] loss: 0.388, acc: 0.867\n",
            "******** [step = 200] loss: 0.387, acc: 0.868\n",
            "EPOCH = 62 loss: 0.387, acc: 0.868, val_loss: 0.376, val_acc: 0.873,test_loss: 0.381, test_acc: 0.872, test1_loss: 0.563, test1_acc: 0.839,test2_loss: 0.732, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:22:10\n",
            "******** [step = 50] loss: 0.386, acc: 0.868\n",
            "******** [step = 100] loss: 0.386, acc: 0.868\n",
            "******** [step = 150] loss: 0.386, acc: 0.868\n",
            "******** [step = 200] loss: 0.385, acc: 0.868\n",
            "EPOCH = 63 loss: 0.385, acc: 0.868, val_loss: 0.374, val_acc: 0.874,test_loss: 0.379, test_acc: 0.873, test1_loss: 0.557, test1_acc: 0.840,test2_loss: 0.722, test2_acc: 0.809\n",
            "\n",
            "================================================================================2021-09_27 09:23:19\n",
            "******** [step = 50] loss: 0.387, acc: 0.868\n",
            "******** [step = 100] loss: 0.386, acc: 0.868\n",
            "******** [step = 150] loss: 0.385, acc: 0.868\n",
            "******** [step = 200] loss: 0.384, acc: 0.869\n",
            "EPOCH = 64 loss: 0.384, acc: 0.868, val_loss: 0.378, val_acc: 0.873,test_loss: 0.384, test_acc: 0.872, test1_loss: 0.568, test1_acc: 0.838,test2_loss: 0.739, test2_acc: 0.806\n",
            "\n",
            "================================================================================2021-09_27 09:24:27\n",
            "******** [step = 50] loss: 0.385, acc: 0.868\n",
            "******** [step = 100] loss: 0.385, acc: 0.868\n",
            "******** [step = 150] loss: 0.384, acc: 0.869\n",
            "******** [step = 200] loss: 0.383, acc: 0.869\n",
            "EPOCH = 65 loss: 0.383, acc: 0.869, val_loss: 0.381, val_acc: 0.873,test_loss: 0.385, test_acc: 0.871, test1_loss: 0.570, test1_acc: 0.838,test2_loss: 0.741, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:25:36\n",
            "******** [step = 50] loss: 0.381, acc: 0.870\n",
            "******** [step = 100] loss: 0.382, acc: 0.870\n",
            "******** [step = 150] loss: 0.380, acc: 0.870\n",
            "******** [step = 200] loss: 0.380, acc: 0.870\n",
            "EPOCH = 66 loss: 0.380, acc: 0.870, val_loss: 0.374, val_acc: 0.875,test_loss: 0.379, test_acc: 0.873, test1_loss: 0.567, test1_acc: 0.839,test2_loss: 0.741, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:26:44\n",
            "******** [step = 50] loss: 0.381, acc: 0.869\n",
            "******** [step = 100] loss: 0.381, acc: 0.869\n",
            "******** [step = 150] loss: 0.380, acc: 0.870\n",
            "******** [step = 200] loss: 0.380, acc: 0.870\n",
            "EPOCH = 67 loss: 0.380, acc: 0.870, val_loss: 0.381, val_acc: 0.872,test_loss: 0.387, test_acc: 0.871, test1_loss: 0.572, test1_acc: 0.837,test2_loss: 0.743, test2_acc: 0.806\n",
            "\n",
            "================================================================================2021-09_27 09:27:52\n",
            "******** [step = 50] loss: 0.379, acc: 0.870\n",
            "******** [step = 100] loss: 0.380, acc: 0.870\n",
            "******** [step = 150] loss: 0.379, acc: 0.870\n",
            "******** [step = 200] loss: 0.378, acc: 0.871\n",
            "EPOCH = 68 loss: 0.378, acc: 0.871, val_loss: 0.374, val_acc: 0.875,test_loss: 0.379, test_acc: 0.873, test1_loss: 0.565, test1_acc: 0.840,test2_loss: 0.736, test2_acc: 0.809\n",
            "\n",
            "================================================================================2021-09_27 09:29:01\n",
            "******** [step = 50] loss: 0.378, acc: 0.871\n",
            "******** [step = 100] loss: 0.379, acc: 0.870\n",
            "******** [step = 150] loss: 0.378, acc: 0.871\n",
            "******** [step = 200] loss: 0.377, acc: 0.871\n",
            "EPOCH = 69 loss: 0.377, acc: 0.871, val_loss: 0.372, val_acc: 0.876,test_loss: 0.377, test_acc: 0.874, test1_loss: 0.562, test1_acc: 0.841,test2_loss: 0.734, test2_acc: 0.809\n",
            "\n",
            "================================================================================2021-09_27 09:30:09\n",
            "******** [step = 50] loss: 0.377, acc: 0.871\n",
            "******** [step = 100] loss: 0.377, acc: 0.871\n",
            "******** [step = 150] loss: 0.375, acc: 0.871\n",
            "******** [step = 200] loss: 0.375, acc: 0.871\n",
            "EPOCH = 70 loss: 0.375, acc: 0.871, val_loss: 0.371, val_acc: 0.876,test_loss: 0.375, test_acc: 0.874, test1_loss: 0.562, test1_acc: 0.841,test2_loss: 0.734, test2_acc: 0.809\n",
            "\n",
            "================================================================================2021-09_27 09:31:18\n",
            "******** [step = 50] loss: 0.374, acc: 0.873\n",
            "******** [step = 100] loss: 0.374, acc: 0.872\n",
            "******** [step = 150] loss: 0.373, acc: 0.872\n",
            "******** [step = 200] loss: 0.372, acc: 0.872\n",
            "EPOCH = 71 loss: 0.372, acc: 0.872, val_loss: 0.377, val_acc: 0.874,test_loss: 0.381, test_acc: 0.873, test1_loss: 0.572, test1_acc: 0.839,test2_loss: 0.748, test2_acc: 0.807\n",
            "\n",
            "================================================================================2021-09_27 09:32:26\n",
            "finishing training...\n",
            "Training complete in 81m 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaZ_laz-Ome7"
      },
      "source": [
        "\n",
        "# gt= [[i[1].split()]for i in test_pairs]\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from nltk.translate.bleu_score import SmoothingFunction\n",
        "# smoothie = SmoothingFunction().method4\n",
        "# total = 0\n",
        "# for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "#         references = gt[i]\n",
        "      \n",
        "#         pred_result, _ = evaluate(transformer, test_pairs[i][0])\n",
        "#         candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split() \n",
        "#         # candidates = tokenzier_decode_char(pred_result, TARG_TEXT.vocab).split() if char-level\n",
        "\n",
        "#         total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "# total*100/len(test_pairs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tBKJj66ktAR"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwj4MOOrdzke"
      },
      "source": [
        "\n",
        "# gt= [[i[1].split()]for i in test_pairs1]\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from nltk.translate.bleu_score import SmoothingFunction\n",
        "# smoothie = SmoothingFunction().method4\n",
        "# total = 0\n",
        "# for i in tqdm(range(len(test_pairs1))):\n",
        "       \n",
        "      \n",
        "#         references = gt[i]\n",
        "      \n",
        "#         pred_result, _ = evaluate(transformer, test_pairs1[i][0])\n",
        "#         candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "#         # candidates = tokenzier_decode_char(pred_result, TARG_TEXT.vocab).split() if char-level\n",
        "\n",
        "#         total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "# total*100/len(test_pairs1)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1m8RB2pd1x1"
      },
      "source": [
        "\n",
        "# gt= [[i[1].split()]for i in test_pairs2]\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from nltk.translate.bleu_score import SmoothingFunction\n",
        "# smoothie = SmoothingFunction().method4\n",
        "# total = 0\n",
        "# for i in tqdm(range(len(test_pairs2))):\n",
        "       \n",
        "      \n",
        "#         references = gt[i]\n",
        "      \n",
        "#         pred_result, _ = evaluate(transformer, test_pairs2[i][0])\n",
        "#         candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "#         # candidates = tokenzier_decode_char(pred_result, TARG_TEXT.vocab).split() if char-level\n",
        "\n",
        "#         total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "# total*100/len(test_pairs2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAYz3Cv1aReL"
      },
      "source": [
        "def batch_translate(pairs,n):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('input:', pair[0])\n",
        "        print('target:', pair[1])\n",
        "        pred_result, _ = evaluate(transformer, pair[0])\n",
        "        # pred_sentence = tokenzier_decode_char(pred_result, TARG_TEXT.vocab) if char-level\n",
        "        pred_sentence = tokenzier_decode1(pred_result, TARG_TEXT.vocab)\n",
        "        print('pred:', pred_sentence)\n",
        "        print('')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ7SA5zwChn1"
      },
      "source": [
        "batch_translate(test_pairs,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0lOTAbhGYCt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}