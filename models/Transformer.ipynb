{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbofDRp5ZAN",
        "outputId": "c83f173a-580f-4c16-b6fe-0b183cc55917"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import nltk as nl\n",
        "from tqdm import tqdm  \n",
        "nl.download('punkt')\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUXSU0AST-r8"
      },
      "source": [
        "The implementation of Transformer is refered to [https://github.com/qingyujean/Magic-NLPer/tree/main/NLP/MachineTranslation/transformer/pytorch] and [https://tensorflow.google.cn/tutorials/text/transformer]. The decription for each part is follow the tutoral of tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHKjbDgD70-B"
      },
      "source": [
        "tokenizer = lambda x: x.split() # word-level\n",
        "# tokenizer = lambda x: list(x) # char-level"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbdeQeBAueI"
      },
      "source": [
        "p_intent = []\n",
        "p_snippet = []\n",
        "c = 0\n",
        "iit = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent:\n",
        "        iit.append(c)\n",
        "      c+=1\n",
        "      p_intent.append(i[:-1].lower())\n",
        "p_intentn = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentn.append(i[:-1].lower())\n",
        "with open('y.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippet.append(i.lower())\n",
        "  \n",
        "train_pairs = [[p_intentn[i],p_snippet[i]] for i in iit]\n",
        "p_intentval = []\n",
        "p_snippetval = []\n",
        "iiv = []\n",
        "c= 0\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intentval:\n",
        "        iiv.append(c)\n",
        "      \n",
        "      p_intentval.append(i[:-1].lower())\n",
        "      c+=1\n",
        "p_intentvaln = []\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentvaln.append(i[:-1].lower())\n",
        "      c+=1\n",
        "with open('yv.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      #i = i.split(' ')\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippetval.append(i.lower())\n",
        "val_pairs = [[p_intentvaln[i],p_snippetval[i]] for i in iiv]\n",
        "p_intenttest = []\n",
        "p_snippettest = []\n",
        "ii = []\n",
        "c= 0\n",
        "with open('xt.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest:\n",
        "        ii.append(c)\n",
        "      c+=1\n",
        "      p_intenttest.append(i[:-1].lower())\n",
        "with open('yt.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippettest.append(i.lower())\n",
        "test_pairs = [[p_intenttest[i],p_snippettest[i]] for i in ii]\n",
        "p_intenttest1 = []\n",
        "p_snippettest1 = []\n",
        "with open('xt1.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest1.append(i[:-1].lower())\n",
        "with open('yt1.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest1.append(i.lower())\n",
        "test_pairs1 = [[p_intenttest1[i],p_snippettest1[i]] for i in ii]\n",
        "p_intenttest2 = []\n",
        "p_snippettest2 = []\n",
        "with open('xt2.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest2.append(i[:-1].lower())\n",
        "with open('yt2.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest2.append(i.lower())\n",
        "test_pairs2 = [[p_intenttest2[i],p_snippettest2[i]] for i in ii]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_N-qCJC9iYA",
        "outputId": "60a73141-eae7-4b85-f8a0-cb69f8747fd9"
      },
      "source": [
        "\n",
        "SRC_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                                tokenize=tokenizer,\n",
        "\n",
        "                                preprocessing=lambda x: ['sos'] + x + ['eos'],\n",
        "                                \n",
        "                                )\n",
        "TARG_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                                 tokenize=tokenizer,\n",
        "\n",
        "                                 preprocessing=lambda x: ['sos'] + x + ['eos'],\n",
        "                                 )\n",
        "                                 \n",
        "\n",
        "\n",
        "def get_dataset(pairs, src, targ):\n",
        "    fields = [('src', src), ('targ', targ)]  \n",
        "    examples = []  \n",
        "    for src, targ in tqdm(pairs): \n",
        "        examples.append(torchtext.legacy.data.Example.fromlist([src, targ], fields))\n",
        "    return examples, fields\n",
        "\n",
        "\n",
        "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
        "\n",
        "ds_train = torchtext.legacy.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
        "ds_val = torchtext.legacy.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
        "ds_test = torchtext.legacy.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n",
        "\n",
        "ds_test1 = torchtext.legacy.data.Dataset(*get_dataset(test_pairs1, SRC_TEXT, TARG_TEXT))\n",
        "ds_test2 = torchtext.legacy.data.Dataset(*get_dataset(test_pairs2, SRC_TEXT, TARG_TEXT))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51942/51942 [00:00<00:00, 95495.82it/s]\n",
            "100%|██████████| 12977/12977 [00:00<00:00, 162502.26it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 56022.07it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 164277.37it/s]\n",
            "100%|██████████| 16095/16095 [00:00<00:00, 47014.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvDPrnG8-sok",
        "outputId": "25467378-5d60-43b0-9a86-a04e8b1bb48c"
      },
      "source": [
        "SRC_TEXT.build_vocab(ds_train) \n",
        "print(len(SRC_TEXT.vocab))\n",
        "print(SRC_TEXT.vocab.itos[0])\n",
        "print(SRC_TEXT.vocab.itos[1])\n",
        "print(SRC_TEXT.vocab.itos[2])\n",
        "print(SRC_TEXT.vocab.itos[3])\n",
        "print(SRC_TEXT.vocab.stoi['sos'])\n",
        "print(SRC_TEXT.vocab.stoi['eos'])\n",
        "\n",
        "\n",
        "TARG_TEXT.build_vocab(ds_train)\n",
        "print(len(TARG_TEXT.vocab))\n",
        "print(TARG_TEXT.vocab.itos[0])\n",
        "print(TARG_TEXT.vocab.itos[1])\n",
        "print(TARG_TEXT.vocab.itos[2])\n",
        "print(TARG_TEXT.vocab.itos[3])\n",
        "print(TARG_TEXT.vocab.stoi['sos'])\n",
        "print(TARG_TEXT.vocab.stoi['eos'])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18911\n",
            "<unk>\n",
            "<pad>\n",
            "eos\n",
            "sos\n",
            "3\n",
            "2\n",
            "19063\n",
            "<unk>\n",
            "<pad>\n",
            "(\n",
            ")\n",
            "6\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQrTIYg2FBK5"
      },
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, data_iter):\n",
        "        self.data_iter = data_iter\n",
        "        self.length = len(data_iter)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __iter__(self):\n",
        "        # switch to batch first\n",
        "        for batch in self.data_iter:\n",
        "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvNMPJJvTtB1"
      },
      "source": [
        "## Positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRSCw9-F0vm"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    # 2*(i//2) make sure that calculating 1/10000^(2i/d)\n",
        "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))\n",
        "    return pos * angle_rates  \n",
        "\n",
        "def positional_encoding(position, d_model):  # d_model = embedding_dim\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  \n",
        "                            np.arange(d_model)[np.newaxis, :], \n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]  \n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLYq74ykUwei"
      },
      "source": [
        "## Masking\n",
        "The padding mask indicates where is a padding token: it outputs a 1 at locations of padding tokens, and a 0 otherwise. It guarantee that the model does not treat padding as the input.\n",
        "\n",
        "The look-ahead mask is used to mask the future tokens in a target sequence. In other words, the mask indicates which target tokens should not be used. For example, to predict the third target token, only the first and second target token will be allowed to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca2NxbI-FKsQ"
      },
      "source": [
        "pad = 1 # When building vocab in Pytorch, index 1 represent padding token \n",
        "def create_padding_mask(seq):  # seq [b, seq_len]\n",
        "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
        "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
        "def create_look_ahead_mask(size):  # seq_len\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
        "    return mask  # [seq_len, seq_len]\n",
        "def create_mask(inp, targ):\n",
        "    # encoder padding mask\n",
        "    # Used in the self-attention in the encoder.\n",
        "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len]\n",
        "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
        "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # combine 2 masks =>[b,1,targ_seq_len,targ_seq_len]\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7P-3DVrVDWh"
      },
      "source": [
        "## Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9TYOdDFUo4"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  #[..., seq_len_q, seq_len_k]\n",
        "    # scale matmul_qk\n",
        "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # depth of k\n",
        "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:  # mask: [batch_size, 1, 1, seq_len]\n",
        "        # The mask is multiplied with -1e9 (close to negative infinity). \n",
        "        # This is done because the mask is summed with the scaled matrix \n",
        "        # multiplication of Q and K and is applied \n",
        "        # immediately before a softmax.\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  \n",
        "\n",
        "    output = torch.matmul(attention_weights, v)  \n",
        "    return output, attention_weights  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT3hi4AmFb2F"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0  #need to make sure each head have same dimension\n",
        "\n",
        "        self.depth = d_model // self.num_heads \n",
        "\n",
        "        self.wq = torch.nn.Linear(d_model, d_model)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads,\n",
        "                   self.depth)  # [b, seq_len, d_model]=>[b, seq_len, num_head, depth]\n",
        "        return x.transpose(1, 2)  # [b, seq_len, num_head, depth]=>[b, num_head, seq_len, depth]\n",
        "\n",
        "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim = d_model\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.wq(q)  # =>[batch_size, seq_len, d_model]\n",
        "        k = self.wk(k)  # =>[batch_size, seq_len, d_model]\n",
        "        v = self.wq(v)  # =>[batch_sizeb, seq_len, d_model]\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        k = self.split_heads(k, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        v = self.split_heads(v, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # => [batch_size, num_head, seq_len_q, depth], [batch_size, num_head, seq_len_q, seq_len_k]\n",
        "\n",
        "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[batch_size, seq_len_q, num_head, depth]\n",
        "        concatenate_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[batch_size, seq_len_q, d_model]\n",
        "\n",
        "        output = self.final_linear(concatenate_attention)  # =>[batch_size, seq_len_q, d_model]\n",
        "        return output, attention_weights  # [batch_size, seq_len_q, d_model], [batch_size, num_head, seq_len_q, seq_len_k]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKDxQ5aNWNyW"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdIytQNfOJlW"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    feed_forward_net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(d_model, dff),  # [batch_size, seq_len, d_model]=>[batch_size, seq_len, dff]\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(dff, d_model),  # [batch_size, seq_len, dff=2048]=>[batch_size, seq_len, d_model]\n",
        "    )\n",
        "    return feed_forward_net"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZCF3FNlWdvC"
      },
      "source": [
        "## Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hjuEDQ4ONcA"
      },
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)  # MultiHeadAttention（padding mask）(self-attention)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, inp_seq_len, embedding_dim] embedding_dim =d_model\n",
        "    # mask [batch_size,1,1,inp_seq_len]\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # =>[batch_size, seq_len, d_model]\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # =>[batch_size, seq_len, d_model]\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  #  =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        return out2  # [batch_size, seq_len, d_model]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x6hgOIjFepX"
      },
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads)  # MultiHeadAttention（look ahead mask and padding mask）(self-attention)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # MultiHeadAttention（padding mask）(encoder-decoder attention)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "        self.dropout3 = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, targ_seq_len, embedding_dim] embedding_dim =d_model\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len] \n",
        "    # enc_output [batch_size, inp_seq_len, d_model]\n",
        "    # padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
        "                                               look_ahead_mask)  # =>[batch_size, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)  #  [batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
        "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
        "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
        "                                               padding_mask)  # =>[batch_size, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(out1 + attn2)  #  [batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # =>[batch_size, targ_seq_len, d_model]\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)  # =>[batch_size, targ_seq_len, d_model]\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "        # [batch_size, targ_seq_len, d_model], [batch_size, num_heads, targ_seq_len, targ_seq_len], [batch_size, num_heads, targ_seq_len, inp_seq_len]\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6MLlHSFoIt"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  \n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 pre_train,\n",
        "                 input_vocab_size, \n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, inp_seq_len]\n",
        "    # mask [batch_size, 1, 1, inp_sel_len]\n",
        "    def forward(self, x, mask):\n",
        "        inp_seq_len = x.shape[-1]\n",
        "\n",
        "        # adding embedding and position encoding\n",
        "        x = self.embedding(x)  # [batch_size, inp_seq_len]=>[batch_size, inp_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
        "        pos_encoding = pos_encoding.cuda()  \n",
        "        x += pos_encoding  # [batch_size, inp_seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)  # [batch_size, inp_seq_len, d_model]=>[batch_size, inp_seq_len, d_model]\n",
        "        return x  # [batch_size, inp_seq_len, d_model]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlgfWG9pORK_"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  # N个encoder layer\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  # dimension of ffn\n",
        "                 target_vocab_size,  # target vocab size\n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
        "        self.pos_encoding = positional_encoding(1000,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "\n",
        "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
        "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [b, targ_seq_len]\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len] \n",
        "    # enc_output [batch_size, inp_seq_len, d_model]\n",
        "    # padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        targ_seq_len = x.shape[-1]\n",
        "\n",
        "        attention_weights = {}\n",
        "\n",
        "        # adding embedding and position encoding\n",
        "        x = self.embedding(x)  # [batch_size, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
        "        pos_encoding = pos_encoding.cuda() #\n",
        "        x += pos_encoding  # [batch_size, inp_seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "            # => [batch_size, targ_seq_len, d_model], [batch_size, num_heads, targ_seq_len, targ_seq_len], [batch_size, num_heads, targ_seq_len, inp_seq_len]\n",
        "\n",
        "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
        "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
        "\n",
        "        return x, attention_weights"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HIqB-SBkEI"
      },
      "source": [
        "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "# real [batch_size, targ_seq_len]\n",
        "# pred [batch_size, targ_seq_len, target_vocab_size]\n",
        "def mask_loss_func(real, pred):\n",
        "    # print(real.shape, pred.shape)\n",
        "    # _loss = loss_object(pred, real) # [batch_size, targ_seq_len]\n",
        "    _loss = loss_object(pred.transpose(-1, -2), real)  # [batch_size, targ_seq_len]\n",
        "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [batch_size, targ_seq_len] \n",
        "    _loss *= mask\n",
        "    return _loss.sum() / mask.sum().item()\n",
        "def mask_accuracy_func(real, pred):\n",
        "    _pred = pred.argmax(dim=-1)  # [batch_size, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
        "    corrects = _pred.eq(real)  # [batch_size, targ_seq_len] bool value\n",
        "    mask = torch.logical_not(real.eq(pad))  # [batch_size, targ_seq_len] where index != 1 \n",
        "    corrects *= mask\n",
        "    return corrects.sum().float() / mask.sum().item()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q35PXjOZ4Ir"
      },
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  # N个encoder layer\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 input_vocab_size,  \n",
        "                 target_vocab_size,  \n",
        "                 pre_train,\n",
        "                 pe_input,  # input max_pos_encoding\n",
        "                 pe_target,\n",
        "                 # input max_pos_encoding\n",
        "                 rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,pre_train,\n",
        "                               input_vocab_size,\n",
        "                               pe_input,\n",
        "                               rate)\n",
        "        self.decoder = Decoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,\n",
        "                               target_vocab_size,\n",
        "                               pe_target,\n",
        "                               rate)\n",
        "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    # inp [batch_size, inp_seq_len]\n",
        "    # targ [batch_size, targ_seq_len]\n",
        "    # enc_padding_mask [batch_size, 1, 1, inp_seq_len]\n",
        "    # look_ahead_mask [batch_size, 1, targ_seq_len, targ_seq_len]\n",
        "    # dec_padding_mask [batch_size, 1, 1, inp_seq_len] #\n",
        "    def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[batch_size, inp_seq_len, d_model]\n",
        "        dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        # => [batch_size, targ_seq_len, d_model],\n",
        "        # {'..block1': [batch_size, num_heads, targ_seq_len, targ_seq_len],\n",
        "        #  '..block2': [batch_size, num_heads, targ_seq_len, inp_seq_len], ...}\n",
        "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFbK6wvlbPRW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HVZxgUzB_Yg"
      },
      "source": [
        "def train_step(model, inp, targ):\n",
        "    # tar_inp as input to decoder due to teacher force\n",
        "    # tar_real is shift 1 prosition of in postion of tar_inp，tar_real include the groudtruth of next predicted token\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
        "\n",
        "    inp = inp.to(device)\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "    # print('device:', inp.device, targ_inp)\n",
        "\n",
        "    model.train()  # set train mode\n",
        "\n",
        "    optimizer.zero_grad()  \n",
        "\n",
        "    # forward\n",
        "    prediction, _ = model(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "    loss = mask_loss_func(targ_real, prediction)\n",
        "    metric = mask_accuracy_func(targ_real, prediction)\n",
        "\n",
        "    # backward\n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "\n",
        "    return loss.item(), metric.item()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bPHR9L2CJ6o"
      },
      "source": [
        "def validate_step(model, inp, targ):\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
        "\n",
        "    inp = inp.to(device)\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "\n",
        "    model.eval()  # set eval mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # forward\n",
        "        prediction, _ = model(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "        val_loss = mask_loss_func(targ_real, prediction)\n",
        "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
        "\n",
        "    return val_loss.item(), val_metric.item()\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTpEoM6HCaoc"
      },
      "source": [
        "EPOCHS = 20 # 50 # 30  # 20\n",
        "\n",
        "print_trainstep_every = 50  # 每50个step做一次打印\n",
        "\n",
        "metric_name = 'acc'\n",
        "\n",
        "def printbar():\n",
        "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
        "    print('\\n' + \"==========\"*8 + '%s'%nowtime)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLUZb_aCjDl"
      },
      "source": [
        "# training function\n",
        "def train_model(model, epochs,ds_train, ds_val ,ds_test,ds_test1,ds_test2, print_every,BATCH_SIZE):\n",
        "    starttime = time.time()\n",
        "    print('*' * 27, 'start training...')\n",
        "    printbar()\n",
        "\n",
        "    best_acc = 0.\n",
        "    # initialize record frame work\n",
        "    df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name,'test_loss', 'test_' + metric_name, 'test1_loss', 'test1_' + metric_name,'test2_loss', 'test2_' + metric_name])\n",
        "    for epoch in range(1,epochs+1):\n",
        "        # laod data\n",
        "        train_iter, val_iter,test_iter,test_iter1,test_iter2 = torchtext.legacy.data.Iterator.splits(\n",
        "    (ds_train, ds_val,ds_test,ds_test1,ds_test2),\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    batch_sizes=(BATCH_SIZE, 256,256,256,256),\n",
        "    shuffle = True\n",
        ")\n",
        "        val_dataloader = DataLoader(val_iter)\n",
        "        test_dataloader = DataLoader(test_iter) \n",
        "        test_dataloader1 = DataLoader(test_iter1)\n",
        "        test_dataloader2 = DataLoader(test_iter2)\n",
        "        train_dataloader = DataLoader(train_iter)\n",
        "        loss_sum = 0.\n",
        "        metric_sum = 0.\n",
        "\n",
        "        for step, (inp, targ) in enumerate(train_dataloader,start = 1):\n",
        "            # inp [64, 10] , targ [64, 10]\n",
        "            loss, metric = train_step(model, inp, targ)\n",
        "\n",
        "            loss_sum += loss\n",
        "            metric_sum += metric\n",
        "            if step % print_every == 0:\n",
        "                print('*' * 8, f'[step = {step}] loss: {loss_sum / step:.3f}, {metric_name}: {metric_sum / step:.3f}')\n",
        "\n",
        "        # adjust lr\n",
        "        scheduler.step()\n",
        "        val_loss_sum = 0.\n",
        "        val_metric_sum = 0.\n",
        "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
        "\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            val_loss_sum += loss\n",
        "            val_metric_sum += metric\n",
        "\n",
        "        test_loss_sum = 0.\n",
        "        test_metric_sum = 0.\n",
        "        for test_step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
        "\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "            test_loss_sum += loss\n",
        "            test_metric_sum += metric\n",
        "        test_loss_sum1 = 0.\n",
        "        test_metric_sum1 = 0.\n",
        "        for test_step1, (inp, targ) in enumerate(test_dataloader1, start=1):\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            test_loss_sum1 += loss\n",
        "            test_metric_sum1 += metric\n",
        "        test_loss_sum2 = 0.\n",
        "        test_metric_sum2 = 0.\n",
        "        bleu = []\n",
        "        for test_step2, (inp, targ) in enumerate(test_dataloader2, start=1):\n",
        "            loss, metric = validate_step(model, inp, targ)\n",
        "\n",
        "            test_loss_sum2 += loss\n",
        "            test_metric_sum2 += metric\n",
        "        if epoch<1000:\n",
        "          record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step,\n",
        "                  test_loss_sum/test_step, test_metric_sum/test_step,test_loss_sum1/test_step1, test_metric_sum1/test_step1,test_loss_sum2/test_step2, test_metric_sum2/test_step2)\n",
        "          df_history.loc[epoch - 1] = record\n",
        "          print('EPOCH = {} loss: {:.3f}, {}: {:.3f}, val_loss: {:.3f}, val_{}: {:.3f},test_loss: {:.3f}, test_{}: {:.3f}, test1_loss: {:.3f}, test1_{}: {:.3f},test2_loss: {:.3f}, test2_{}: {:.3f}'.format(\n",
        "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4],record[5],metric_name, record[6],record[7],metric_name,record[8],record[9],metric_name,record[10]))\n",
        "          printbar()\n",
        "          # early stop\n",
        "          if len(df_history)>=11:\n",
        "            if record[3]>=df_history.loc[epoch - 11][3]:\n",
        "              break\n",
        "          else:\n",
        "            continue\n",
        "    print('finishing training...')\n",
        "    endtime = time.time()\n",
        "    time_elapsed = endtime - starttime\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    return df_history\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POIzwADYcQX9"
      },
      "source": [
        "## Generating FOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zf6tNjsG5Ew"
      },
      "source": [
        "def tokenizer_encode(tokenize, sentence, vocab):\n",
        "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
        "    # print(len(vocab))\n",
        "    sentence =sentence.lower()\n",
        "    # print(type(sentence)) # str\n",
        "    sentence = tokenize(sentence)  # list\n",
        "    sentence = ['sos'] + sentence + ['eos']\n",
        "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
        "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
        "    return sentence_ids\n",
        "\n",
        "\n",
        "\n",
        "def tokenzier_decode1(sentence_ids, vocab):\n",
        "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
        "    # print(sentence)\n",
        "    return \" \".join(sentence[1:])\n",
        "\n",
        "\n",
        "def evaluate(model, inp_sentence):\n",
        "    model.eval()  # 设置eval mode\n",
        "\n",
        "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
        "    # print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
        "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[batch_size=1, inp_seq_len=10]\n",
        "    # print(encoder_input.shape)\n",
        "    decoder_input = [TARG_TEXT.vocab.stoi['sos']]\n",
        "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[batch_size=1,seq_len=1]\n",
        "    # print(decoder_input.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu()) ################\n",
        "        encoder_input = encoder_input.to(device)\n",
        "        enc_padding_mask = enc_padding_mask.to(device)\n",
        "        enout = model.encoder(encoder_input, enc_padding_mask)\n",
        "        for i in range(200 + 2):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu()) ################\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            combined_mask = combined_mask.to(device)\n",
        "            dec_padding_mask = dec_padding_mask.to(device)\n",
        "            predictions, attention_weights = model.decoder(decoder_input, enout, combined_mask, dec_padding_mask)\n",
        "            predictions = model.final_layer(predictions)\n",
        "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
        "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
        "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['eos']:\n",
        "                return decoder_input.squeeze(dim=0), attention_weights\n",
        "\n",
        "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
        "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
        "            # decoder_input being longer\n",
        "    return decoder_input.squeeze(dim=0), attention_weights\n",
        "    # [targ_seq_len],\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnFzAdyOCt7f",
        "outputId": "05ac6ecf-872f-4987-ad91-7be0e4c33bc2"
      },
      "source": [
        "num_layers = 3\n",
        "d_model =300\n",
        "dff = 600\n",
        "num_heads = 5\n",
        "\n",
        "input_vocab_size = len(SRC_TEXT.vocab) # 3901\n",
        "target_vocab_size = len(TARG_TEXT.vocab) # 2591\n",
        "dropout_rate = 0.3\n",
        "torch.cuda.empty_cache()\n",
        "transformer = Transformer(num_layers,\n",
        "                          d_model,\n",
        "                          num_heads,\n",
        "                          dff,\n",
        "                          input_vocab_size,\n",
        "                          target_vocab_size,\n",
        "                          None,\n",
        "                          pe_input=input_vocab_size,#set 1000 if char-level\n",
        "                          pe_target=target_vocab_size,#set 1000 if char-level\n",
        "                          \n",
        "                          rate=dropout_rate)\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(),lr=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer , 5,\n",
        "                gamma =0.9)\n",
        "df_history = train_model(transformer, 10, ds_train, ds_val, ds_test,ds_test1,ds_test2,print_trainstep_every,256)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************** start training...\n",
            "\n",
            "================================================================================2021-09_26 18:55:35\n",
            "******** [step = 50] loss: 3.776, acc: 0.513\n",
            "******** [step = 100] loss: 2.971, acc: 0.593\n",
            "******** [step = 150] loss: 2.600, acc: 0.642\n",
            "******** [step = 200] loss: 2.370, acc: 0.678\n",
            "EPOCH = 1 loss: 2.359, acc: 0.680, val_loss: 1.699, val_acc: 0.792,test_loss: 1.703, test_acc: 0.791, test1_loss: 1.761, test1_acc: 0.780,test2_loss: 1.811, test2_acc: 0.770\n",
            "\n",
            "================================================================================2021-09_26 18:56:35\n",
            "******** [step = 50] loss: 1.552, acc: 0.803\n",
            "******** [step = 100] loss: 1.512, acc: 0.810\n",
            "******** [step = 150] loss: 1.481, acc: 0.814\n",
            "******** [step = 200] loss: 1.459, acc: 0.817\n",
            "EPOCH = 2 loss: 1.458, acc: 0.817, val_loss: 1.503, val_acc: 0.816,test_loss: 1.509, test_acc: 0.816, test1_loss: 1.592, test1_acc: 0.804,test2_loss: 1.665, test2_acc: 0.793\n",
            "\n",
            "================================================================================2021-09_26 18:57:35\n",
            "******** [step = 50] loss: 1.374, acc: 0.824\n",
            "******** [step = 100] loss: 1.354, acc: 0.828\n",
            "******** [step = 150] loss: 1.334, acc: 0.831\n",
            "******** [step = 200] loss: 1.318, acc: 0.833\n",
            "EPOCH = 3 loss: 1.317, acc: 0.833, val_loss: 1.384, val_acc: 0.834,test_loss: 1.390, test_acc: 0.833, test1_loss: 1.488, test1_acc: 0.820,test2_loss: 1.577, test2_acc: 0.808\n",
            "\n",
            "================================================================================2021-09_26 18:58:35\n",
            "******** [step = 50] loss: 1.234, acc: 0.843\n",
            "******** [step = 100] loss: 1.217, acc: 0.846\n",
            "******** [step = 150] loss: 1.194, acc: 0.849\n",
            "******** [step = 200] loss: 1.175, acc: 0.851\n",
            "EPOCH = 4 loss: 1.174, acc: 0.851, val_loss: 1.224, val_acc: 0.856,test_loss: 1.230, test_acc: 0.855, test1_loss: 1.347, test1_acc: 0.840,test2_loss: 1.459, test2_acc: 0.826\n",
            "\n",
            "================================================================================2021-09_26 18:59:36\n",
            "******** [step = 50] loss: 1.069, acc: 0.863\n",
            "******** [step = 100] loss: 1.048, acc: 0.866\n",
            "******** [step = 150] loss: 1.024, acc: 0.868\n",
            "******** [step = 200] loss: 1.010, acc: 0.869\n",
            "EPOCH = 5 loss: 1.010, acc: 0.869, val_loss: 1.059, val_acc: 0.874,test_loss: 1.063, test_acc: 0.873, test1_loss: 1.213, test1_acc: 0.855,test2_loss: 1.362, test2_acc: 0.836\n",
            "\n",
            "================================================================================2021-09_26 19:00:36\n",
            "******** [step = 50] loss: 0.910, acc: 0.879\n",
            "******** [step = 100] loss: 0.886, acc: 0.882\n",
            "******** [step = 150] loss: 0.862, acc: 0.884\n",
            "******** [step = 200] loss: 0.843, acc: 0.886\n",
            "EPOCH = 6 loss: 0.842, acc: 0.886, val_loss: 0.892, val_acc: 0.894,test_loss: 0.896, test_acc: 0.893, test1_loss: 1.072, test1_acc: 0.874,test2_loss: 1.244, test2_acc: 0.855\n",
            "\n",
            "================================================================================2021-09_26 19:01:36\n",
            "******** [step = 50] loss: 0.757, acc: 0.895\n",
            "******** [step = 100] loss: 0.740, acc: 0.896\n",
            "******** [step = 150] loss: 0.720, acc: 0.898\n",
            "******** [step = 200] loss: 0.705, acc: 0.899\n",
            "EPOCH = 7 loss: 0.704, acc: 0.899, val_loss: 0.778, val_acc: 0.905,test_loss: 0.782, test_acc: 0.905, test1_loss: 0.985, test1_acc: 0.883,test2_loss: 1.181, test2_acc: 0.863\n",
            "\n",
            "================================================================================2021-09_26 19:02:36\n",
            "******** [step = 50] loss: 0.633, acc: 0.906\n",
            "******** [step = 100] loss: 0.623, acc: 0.906\n",
            "******** [step = 150] loss: 0.622, acc: 0.904\n",
            "******** [step = 200] loss: 0.604, acc: 0.906\n",
            "EPOCH = 8 loss: 0.603, acc: 0.906, val_loss: 0.690, val_acc: 0.914,test_loss: 0.692, test_acc: 0.914, test1_loss: 0.910, test1_acc: 0.892,test2_loss: 1.119, test2_acc: 0.871\n",
            "\n",
            "================================================================================2021-09_26 19:03:36\n",
            "******** [step = 50] loss: 0.527, acc: 0.916\n",
            "******** [step = 100] loss: 0.515, acc: 0.917\n",
            "******** [step = 150] loss: 0.500, acc: 0.918\n",
            "******** [step = 200] loss: 0.489, acc: 0.920\n",
            "EPOCH = 9 loss: 0.488, acc: 0.920, val_loss: 0.628, val_acc: 0.921,test_loss: 0.632, test_acc: 0.920, test1_loss: 0.859, test1_acc: 0.899,test2_loss: 1.083, test2_acc: 0.877\n",
            "\n",
            "================================================================================2021-09_26 19:04:36\n",
            "******** [step = 50] loss: 0.437, acc: 0.925\n",
            "******** [step = 100] loss: 0.428, acc: 0.925\n",
            "******** [step = 150] loss: 0.417, acc: 0.926\n",
            "******** [step = 200] loss: 0.407, acc: 0.928\n",
            "EPOCH = 10 loss: 0.406, acc: 0.928, val_loss: 0.579, val_acc: 0.923,test_loss: 0.583, test_acc: 0.922, test1_loss: 0.832, test1_acc: 0.899,test2_loss: 1.077, test2_acc: 0.877\n",
            "\n",
            "================================================================================2021-09_26 19:05:37\n",
            "finishing training...\n",
            "Training complete in 10m 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP2Xpjj9migu"
      },
      "source": [
        "# df_history = train_model(transformer, 1000, ds_train, ds_val, ds_test,ds_test1,ds_test2,print_trainstep_every,256)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaZ_laz-Ome7"
      },
      "source": [
        "\n",
        "# gt= [[i[1].split()]for i in test_pairs]\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from nltk.translate.bleu_score import SmoothingFunction\n",
        "# smoothie = SmoothingFunction().method4\n",
        "# total = 0\n",
        "# for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "#         references = gt[i]\n",
        "      \n",
        "#         pred_result, _ = evaluate(transformer, test_pairs[i][0])\n",
        "#         candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "#         total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "# total*100/len(test_pairs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tBKJj66ktAR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwj4MOOrdzke"
      },
      "source": [
        "\n",
        "gt= [[i[1].split()]for i in test_pairs1]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs1))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        pred_result, _ = evaluate(transformer, test_pairs1[i][0])\n",
        "        candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1m8RB2pd1x1"
      },
      "source": [
        "\n",
        "gt= [[i[1].split()]for i in test_pairs2]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs2))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        pred_result, _ = evaluate(transformer, test_pairs2[i][0])\n",
        "        candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAYz3Cv1aReL"
      },
      "source": [
        "def batch_translate(pairs,n):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('input:', pair[0])\n",
        "        print('target:', pair[1])\n",
        "        pred_result, _ = evaluate(transformer, pair[0])\n",
        "        pred_sentence = tokenzier_decode1(pred_result, TARG_TEXT.vocab)\n",
        "        print('pred:', pred_sentence)\n",
        "        print('')\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ7SA5zwChn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7469be-8eee-46d7-de7b-e14ea747e25b"
      },
      "source": [
        "batch_translate(test_pairs,10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: dwight has at least one day\n",
            "target: ∃x ( day ( x , dwight ) )\n",
            "pred: ∃x ( day ( x , moore ) )\n",
            "\n",
            "input: every top who is a girl must be a period\n",
            "target: ∀x ( ( top ( x ) & girl ( x ) ) -> period ( x ) )\n",
            "pred: ∀x ( ( top ( x ) & top ( x ) ) -> top ( x ) )\n",
            "\n",
            "input: there exists a earthenware\n",
            "target: ∃x earthenware ( x )\n",
            "pred: ∃x ( will ( x ) & will ( x ) )\n",
            "\n",
            "input: not every rhythm who believe feeling center part\n",
            "target: - ∀x ( ( rhythm ( x ) & believe ( x , feeling ) ) -> center ( x , part ) )\n",
            "pred: - ∀x ( ( part ( x ) & believe ( x , rhythm ) ) -> believe ( x , part ) )\n",
            "\n",
            "input: all private one-story are not basis\n",
            "target: ∀x ( ( one-story ( x ) & private ( x ) ) -> - basis ( x ) )\n",
            "pred: ∀x ( ( private ( x ) & private ( x ) ) -> - basis ( x ) )\n",
            "\n",
            "input: frenchman has exactly one institution\n",
            "target: ∃x ( institution ( x , frenchman ) & ∀y ( institution ( y , frenchman ) -> x == y ) )\n",
            "pred: ∃x ( institution ( x , frenchman ) & ∀y ( institution ( y , frenchman ) -> x == y ) )\n",
            "\n",
            "input: everybody stand a onset who cancel <ppl>\n",
            "target: ∀x ∀y ( ( onset ( y ) & cancel ( y , y ) )-> stand ( x , y ) )\n",
            "pred: ∀x ∀y ( ( onset ( y ) & stand ( y , y ) )-> stand ( x , y ) )\n",
            "\n",
            "input: everyone who hazard a rasp compete a freedom\n",
            "target: ∀x ( ∃y ( rasp ( y ) & hazard ( x , y ) ) -> ∃z ( compete ( x , z ) ) )\n",
            "pred: ∀x ( ∃y ( freedom ( y ) & compete ( x , y ) ) -> ∃z ( compete ( x , z ) ) )\n",
            "\n",
            "input: some substance are not secret and own\n",
            "target: ∃x ( substance ( x ) & - ( secret ( x ) & own ( x ) ) )\n",
            "pred: ∃x ( substance ( x ) & - ( own ( x ) & own ( x ) ) )\n",
            "\n",
            "input: for every man there is a language which is further than it\n",
            "target: ∀x ( man ( x ) -> ∃y ( language ( y ) & further ( y, x ) ) )\n",
            "pred: ∀x ( language ( x ) -> ∃y ( language ( y ) & further ( y, x ) ) )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0lOTAbhGYCt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}