{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Dual-encoders_Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbofDRp5ZAN",
        "outputId": "05b84a6d-4b86-4f7a-96c6-e19788529b75"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import nltk as nl\n",
        "from tqdm import tqdm  \n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import gensim.downloader as api\n",
        "! pip install pytorch-pretrained-bert\n",
        "! pip install transformers\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.18.48)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.48 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.21.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.48->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAkWUW0t1Jom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ba8fef-ec92-4052-a965-f5702d5294e9"
      },
      "source": [
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "# load BERT tokenizer\n",
        "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbdeQeBAueI"
      },
      "source": [
        "\n",
        "p_intent = []\n",
        "p_snippet = []\n",
        "c = 0\n",
        "iit = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent:\n",
        "        iit.append(c)\n",
        "      c+=1\n",
        "      p_intent.append(i[:-1].lower())\n",
        "p_intentn = []\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentn.append(i[:-1].lower())\n",
        "with open('y.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippet.append(i.lower())\n",
        "  \n",
        "train_pairs = [[p_intentn[i],p_snippet[i]] for i in iit]\n",
        "p_intentval = []\n",
        "p_snippetval = []\n",
        "iiv = []\n",
        "c= 0\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intentval:\n",
        "        iiv.append(c)\n",
        "      \n",
        "      p_intentval.append(i[:-1].lower())\n",
        "      c+=1\n",
        "p_intentvaln = []\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentvaln.append(i[:-1].lower())\n",
        "      c+=1\n",
        "with open('yv.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      #i = i.split(' ')\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippetval.append(i.lower())\n",
        "val_pairs = [[p_intentvaln[i],p_snippetval[i]] for i in iiv]\n",
        "p_intenttest = []\n",
        "p_snippettest = []\n",
        "ii = []\n",
        "c= 0\n",
        "with open('xt.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest:\n",
        "        ii.append(c)\n",
        "      c+=1\n",
        "      p_intenttest.append(i[:-1].lower())\n",
        "with open('yt.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippettest.append(i.lower())\n",
        "test_pairs = [[p_intenttest[i],p_snippettest[i]] for i in ii]\n",
        "p_intenttest1 = []\n",
        "p_snippettest1 = []\n",
        "with open('xt1.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest1.append(i[:-1].lower())\n",
        "with open('yt1.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest1.append(i.lower())\n",
        "test_pairs1 = [[p_intenttest1[i],p_snippettest1[i]] for i in ii]\n",
        "p_intenttest2 = []\n",
        "p_snippettest2 = []\n",
        "with open('xt2.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest2.append(i[:-1].lower())\n",
        "with open('yt2.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest2.append(i.lower())\n",
        "test_pairs2 = [[p_intenttest2[i],p_snippettest2[i]] for i in ii]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zzQ2JwJbsVv"
      },
      "source": [
        "# load extra training data\n",
        "# p_intentextra = []\n",
        "# p_snippetextra = []\n",
        "# c = 0\n",
        "# iite = []\n",
        "# with open('xextra.txt', 'r',encoding='UTF-8') as intent:\n",
        "#     for i in intent:\n",
        "#       if i[:-1].lower() not in p_intentextra and i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest and i[:-1].lower() not in p_intentval:\n",
        "#         iite.append(c)\n",
        "#       c+=1\n",
        "#       p_intentextra.append(i[:-1].lower())\n",
        "\n",
        "# with open('yextra.txt', 'r',encoding='UTF-8') as snippet:\n",
        "#     for i in snippet:\n",
        "#       p_snippetextra.append(i[:-1].lower())\n",
        "# extratrain = [[p_intentextra[i],p_snippetextra[i]] for i in iite]\n",
        "# train_pairs = train_pairs+extratrain"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXcRoUwHSYne"
      },
      "source": [
        "# use BERTtokenizer tokenize the input\n",
        "k = [['[CLS]'] + tz.tokenize(x[0]) + ['[SEP]'] for x in train_pairs]\n",
        "kk = [['[CLS]'] +tz.tokenize(x[0]) + ['[SEP]'] for x in val_pairs]\n",
        "kkk = [['[CLS]'] + tz.tokenize(x[0]) + ['[SEP]'] for x in test_pairs]\n",
        "kkk1 = [['[CLS]'] + tz.tokenize(x[0]) + ['[SEP]'] for x in test_pairs1]\n",
        "kkk2 = [['[CLS]'] + tz.tokenize(x[0]) + ['[SEP]'] for x in test_pairs2]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSQhsvjF0vVm"
      },
      "source": [
        "# convert to index according to BERT vocab\n",
        "enco = [torch.tensor(tz.convert_tokens_to_ids(x)) for x in k]\n",
        "encoval =[torch.tensor(tz.convert_tokens_to_ids(x)) for x in kk]\n",
        "encotest = [torch.tensor(tz.convert_tokens_to_ids(x)) for x in kkk]\n",
        "encotest1 = [torch.tensor(tz.convert_tokens_to_ids(x)) for x in kkk1]\n",
        "encotest2 = [torch.tensor(tz.convert_tokens_to_ids(x)) for x in kkk2]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_N-qCJC9iYA"
      },
      "source": [
        "tokenizer_for_tar = lambda x: x.split() # 分词器\n",
        "MAX_LENGTH = 196\n",
        "SRC_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "\n",
        "                                )\n",
        "TARG_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "              \n",
        "                                 )\n",
        "\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0M0PtloAU2"
      },
      "source": [
        "# use normal tokenizer tokenize the label with white space seperator\n",
        "t  = [['[CLS]'] + x[1].split() + ['[SEP]'] for x in train_pairs]\n",
        "tt = [['[CLS]'] + x[1].split() + ['[SEP]'] for x in val_pairs]\n",
        "ttt = [['[CLS]'] + x[1].split() + ['[SEP]'] for x in test_pairs]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_iDF890obWu",
        "outputId": "ebcd11d1-6e5e-4f4b-84a1-d4aec4e24600"
      },
      "source": [
        "# build vocab for encoder\n",
        "SRC_TEXT.build_vocab(k)  \n",
        "print(len(SRC_TEXT.vocab))\n",
        "print(SRC_TEXT.vocab.itos[0])\n",
        "print(SRC_TEXT.vocab.itos[1])\n",
        "print(SRC_TEXT.vocab.itos[2])\n",
        "print(SRC_TEXT.vocab.itos[3])\n",
        "print(SRC_TEXT.vocab.stoi['[CLS]'])\n",
        "print(SRC_TEXT.vocab.stoi['[SEP]'])\n",
        "# build vocab for decoder\n",
        "TARG_TEXT.build_vocab(t)\n",
        "print(len(TARG_TEXT.vocab))\n",
        "print(TARG_TEXT.vocab.itos[0])\n",
        "print(TARG_TEXT.vocab.itos[1])\n",
        "print(TARG_TEXT.vocab.itos[2])\n",
        "print(TARG_TEXT.vocab.itos[3])\n",
        "print(TARG_TEXT.vocab.stoi['[CLS]'])\n",
        "print(TARG_TEXT.vocab.stoi['[SEP]'])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11040\n",
            "<unk>\n",
            "<pad>\n",
            "[CLS]\n",
            "[SEP]\n",
            "2\n",
            "3\n",
            "19063\n",
            "<unk>\n",
            "<pad>\n",
            "(\n",
            ")\n",
            "5\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JqgiXm4TNsg"
      },
      "source": [
        "\n",
        "# convert to index according to encoder vocab\n",
        "train_i = [[SRC_TEXT.vocab.stoi[x] for x in i] for i in k]\n",
        "val_i = [[SRC_TEXT.vocab.stoi[x] for x in i] for i in kk]\n",
        "test_i = [[SRC_TEXT.vocab.stoi[x] for x in i] for i in kkk]\n",
        "test_i1 = [[SRC_TEXT.vocab.stoi[x] for x in i] for i in kkk1]\n",
        "test_i2 = [[SRC_TEXT.vocab.stoi[x] for x in i] for i in kkk2]\n",
        "final_trains_i = [torch.tensor(i) for i in train_i]\n",
        "final_val_i = [torch.tensor(i) for i in val_i]\n",
        "final_test_i = [torch.tensor(i) for i in test_i]\n",
        "final_test_i1 = [torch.tensor(i) for i in test_i1]\n",
        "final_test_i2 = [torch.tensor(i) for i in test_i2]\n",
        "\n",
        "# convert to index according to decoder vocab\n",
        "train_s = [[TARG_TEXT.vocab.stoi[x] for x in i] for i in t]\n",
        "val_s = [[TARG_TEXT.vocab.stoi[x] for x in i] for i in tt]\n",
        "test_s = [[TARG_TEXT.vocab.stoi[x] for x in i] for i in ttt]\n",
        "final_trains = [torch.tensor(i) for i in train_s]\n",
        "final_val_s = [torch.tensor(i) for i in val_s]\n",
        "final_test_s = [torch.tensor(i) for i in test_s]\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4au-1Lk5KzN",
        "outputId": "ebab46b8-bde3-4325-ac55-61a899a44dda"
      },
      "source": [
        "print(len(final_trains))\n",
        "print(len(final_trains) == len(enco))\n",
        "print(len(final_test_s))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51942\n",
            "True\n",
            "16095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gGHCVn9rpZt"
      },
      "source": [
        "# for collecting data and padding data during mini-batch learning\n",
        "def collate_fn(data):\n",
        "    bertin, inputs,targ = zip(*data)\n",
        "    bertin = torch.nn.utils.rnn.pad_sequence(bertin, batch_first=True, padding_value=0)\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=1)\n",
        "    targ = torch.nn.utils.rnn.pad_sequence(targ, batch_first=True, padding_value=1)\n",
        "    return bertin,inputs,targ\n",
        "datat = list(zip(enco,final_trains_i,final_trains))\n",
        "d = torch.utils.data.DataLoader(datat, batch_size = 4,shuffle = False,collate_fn= collate_fn)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXU6-8FhsxhJ",
        "outputId": "501b36a0-43f2-4a0a-c51b-3ba713605e05"
      },
      "source": [
        "for step, (b,i,t) in enumerate(d):\n",
        "  with torch.no_grad():\n",
        "      print(b)\n",
        "      print(i)\n",
        "      # enc_output = bret(i.to(device))[0]\n",
        "  break\n",
        "step"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1175,  1674,  1136,  4056,   170,  2458,   102],\n",
            "        [  101,  1175,  1674,  1136,  4056,   170,  2616,   102],\n",
            "        [  101,  1175,  1674,  1136,  4056,   170,  1411,   102],\n",
            "        [  101,  1175,  1674,  1136,  4056,   170, 12629,   102]])\n",
            "tensor([[   2,   14,   57,    9,   50,    6,  460,    3],\n",
            "        [   2,   14,   57,    9,   50,    6,  290,    3],\n",
            "        [   2,   14,   57,    9,   50,    6,  315,    3],\n",
            "        [   2,   14,   57,    9,   50,    6, 5600,    3]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6waSOZkQmiak"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    # 2*(i//2) make sure that calculating 1/10000^(2i/d)\n",
        "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))\n",
        "    return pos * angle_rates  # [50,1]*[1,512]=>[50, 512]\n",
        "\n",
        "def positional_encoding(position, d_model):  # d_model = embedding_dim\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  \n",
        "                            np.arange(d_model)[np.newaxis, :], \n",
        "                            d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]  \n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRSCw9-F0vm"
      },
      "source": [
        "def create_padding_mask(seq,pad):  # seq [b, seq_len]\n",
        "    seq = torch.eq(seq, torch.tensor(pad)).float() \n",
        "    return seq[:, np.newaxis, np.newaxis, :]# =>[b, 1, 1, seq_len]\n",
        "\n",
        "def create_look_ahead_mask(size):  # seq_len\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
        "    return mask  # [seq_len, seq_len]\n",
        "def create_mask(bret_inp,inp, targ):\n",
        "    # encoder padding mask\n",
        "    # Used in the self-attention in the encoder.\n",
        "    enc_padding_mask = create_padding_mask(inp,1)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # used in decoder's encoder-decoder attention\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp,1)  # =>[b,1,1,inp_seq_len] \n",
        "    \n",
        "    # Used in the self-attention in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len]\n",
        "    dec_targ_padding_mask = create_padding_mask(targ,1)  # =>[b,1,1,targ_seq_len]\n",
        "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # combine 2 masks =>[b,1,targ_seq_len,targ_seq_len]\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask,create_padding_mask(bret_inp,0) #The last one is BERT padding mask, and in BERT vocab, index 0 is mean the padding"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9TYOdDFUo4"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  #[..., seq_len_q, seq_len_k]\n",
        "    # scale matmul_qk\n",
        "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # depth of k\n",
        "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:  # mask: [batch_size, 1, 1, seq_len]\n",
        "        # The mask is multiplied with -1e9 (close to negative infinity). \n",
        "        # This is done because the mask is summed with the scaled matrix \n",
        "        # multiplication of Q and K and is applied \n",
        "        # immediately before a softmax.\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  \n",
        "\n",
        "    output = torch.matmul(attention_weights, v)  \n",
        "    return output, attention_weights  "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT3hi4AmFb2F"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0  #need to make sure each head have same dimension\n",
        "\n",
        "        self.depth = d_model // self.num_heads \n",
        "\n",
        "        self.wq = torch.nn.Linear(d_model, d_model)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads,\n",
        "                   self.depth)  # [b, seq_len, d_model]=>[b, seq_len, num_head, depth]\n",
        "        return x.transpose(1, 2)  # [b, seq_len, num_head, depth]=>[b, num_head, seq_len, depth]\n",
        "\n",
        "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim = d_model\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.wq(q)  # =>[batch_size, seq_len, d_model]\n",
        "        k = self.wk(k)  # =>[batch_size, seq_len, d_model]\n",
        "        v = self.wq(v)  # =>[batch_sizeb, seq_len, d_model]\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        k = self.split_heads(k, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        v = self.split_heads(v, batch_size)  # =>[batch_size, num_head, seq_len, depth]\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # => [batch_size, num_head, seq_len_q, depth], [batch_size, num_head, seq_len_q, seq_len_k]\n",
        "\n",
        "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[batch_size, seq_len_q, num_head, depth]\n",
        "        concatenate_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[batch_size, seq_len_q, d_model]\n",
        "\n",
        "        output = self.final_linear(concatenate_attention)  # =>[batch_size, seq_len_q, d_model]\n",
        "        return output, attention_weights  # [batch_size, seq_len_q, d_model], [batch_size, num_head, seq_len_q, seq_len_k]\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdIytQNfOJlW"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    feed_forward_net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(d_model, dff),  # [b, seq_len, d_model]=>[b, seq_len, dff=2048]\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(dff, d_model),  # [b, seq_len, dff=2048]=>[b, seq_len, d_model=512]\n",
        "    )\n",
        "    return feed_forward_net\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hjuEDQ4ONcA"
      },
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)  # MultiHeadAttention（padding mask）(self-attention)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [batch_size, inp_seq_len, embedding_dim] embedding_dim =d_model\n",
        "    # mask [batch_size,1,1,inp_seq_len]\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # =>[batch_size, seq_len, d_model]\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # =>[batch_size, seq_len, d_model]\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  #  =>[batch_size, seq_len, d_model]\n",
        "\n",
        "        return out2  # [batch_size, seq_len, d_model]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x6hgOIjFepX"
      },
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,num_heads)  # (self-attention)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # (encoder-decoder attention)\n",
        "        self.mha3 = MultiHeadAttention(d_model, num_heads)# (BERT-decoder attention)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.mapp = nn.Linear(768,d_model)# Mapping funtion\n",
        "        self.rrr = torch.nn.ReLU()\n",
        "        self.dropout1 = torch.nn.Dropout(rate)\n",
        "        self.dropout2 = torch.nn.Dropout(rate)\n",
        "        self.dropout3 = torch.nn.Dropout(rate)\n",
        "        self.dropout4 = torch.nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask,bert_out,b_m):\n",
        "        # bert_out =  self.mapp(bert_out)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
        "                                               look_ahead_mask) \n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1) \n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
        "                                               padding_mask) \n",
        "        attn2 = self.dropout2(attn2)\n",
        "        # bret_atten ,attn_weights_block3 =  self.mha3(out1, self.rrr(self.mapp(bert_out)), self.rrr(self.mapp(bert_out)), b_m)  # calculate BERT-decoder attention, if d_model is not match the dim of BERT, use a ReLU and linear function map the dim of BERT to d_model\n",
        "        bret_atten ,attn_weights_block3 =  self.mha3(out1, bert_out, bert_out, b_m)  # calculate BERT-decoder attention\n",
        "        bret_atten = self.dropout3(bret_atten)\n",
        "        # average encoder-decoder attention and BERT-decoder attention\n",
        "        attn3 = (attn2+bret_atten)/2\n",
        "        out2 = self.layernorm2(out1 + attn3) \n",
        "        ffn_output = self.ffn(out2)  # =>[b, targ_seq_len, d_model]\n",
        "        ffn_output = self.dropout4(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output) \n",
        "\n",
        "        return out3, attn_weights_block1, (attn_weights_block2+attn_weights_block3)/2"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-VfRJLfVUBD"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  # N个encoder layer\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 pre_train,\n",
        "                 input_vocab_size,  \n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        # self.embedding.weight.data.copy_(torch.from_numpy(pre_train))\n",
        "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "\n",
        "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
        "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    # x [b, inp_seq_len]\n",
        "    # mask [b, 1, 1, inp_sel_len]\n",
        "    def forward(self, x, mask):\n",
        "        inp_seq_len = x.shape[-1]\n",
        "\n",
        "        # adding embedding and position encoding\n",
        "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
        "        pos_encoding = pos_encoding.cuda()  # ###############\n",
        "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
        "        return x  # [b, inp_seq_len, d_model]\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlgfWG9pORK_"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  \n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 target_vocab_size,  \n",
        "                 maximun_position_encoding,\n",
        "                 rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
        "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
        "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
        "\n",
        "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask,bertout,b_m):\n",
        "        targ_seq_len = x.shape[-1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
        "        pos_encoding = pos_encoding.cuda() # ###############\n",
        "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.num_layers):\n",
        "            # compare to vanilla decoder, additionally feed the BERT padding mask and BERT output\n",
        "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask,bertout,b_m)\n",
        "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
        "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
        "\n",
        "        return x, attention_weights"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1yztAquV9xS"
      },
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,  \n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,  \n",
        "                 input_vocab_size, \n",
        "                 target_vocab_size,  \n",
        "                 pe_input,  # input max_pos_encoding\n",
        "                 pe_target,\n",
        "                 # input max_pos_encoding\n",
        "                 rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        # initialize BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "        self.encoder = Encoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,\n",
        "                               input_vocab_size,\n",
        "                               pe_input,\n",
        "                               rate)\n",
        "        self.decoder = Decoder(num_layers,\n",
        "                               d_model,\n",
        "                               num_heads,\n",
        "                               dff,\n",
        "                               target_vocab_size,\n",
        "                               pe_target,\n",
        "                               rate)\n",
        "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, bertinp, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask,b_m):\n",
        "        # get output of last hidden layer of BERT, and we don't fine-tune BERT in this project\n",
        "        with torch.no_grad():\n",
        "          bertout = self.bert(bertinp)[0][-1]\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
        "        # additionally feed the BERT padding mask and BERT output\n",
        "        dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask,bertout,b_m)\n",
        "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HIqB-SBkEI"
      },
      "source": [
        "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "# real [batch_size, targ_seq_len]\n",
        "# pred [batch_size, targ_seq_len, target_vocab_size]\n",
        "def mask_loss_func(real, pred,pad):\n",
        "    # print(real.shape, pred.shape)\n",
        "    # _loss = loss_object(pred, real) # [batch_size, targ_seq_len]\n",
        "    _loss = loss_object(pred.transpose(-1, -2), real)  # [batch_size, targ_seq_len]\n",
        "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [batch_size, targ_seq_len] \n",
        "    _loss *= mask\n",
        "    return _loss.sum() / mask.sum().item()\n",
        "def mask_accuracy_func(real, pred,pad):\n",
        "    _pred = pred.argmax(dim=-1)  # [batch_size, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
        "    corrects = _pred.eq(real)  # [batch_size, targ_seq_len] bool value\n",
        "    mask = torch.logical_not(real.eq(pad))  # [batch_size, targ_seq_len] where index != 1 \n",
        "    corrects *= mask\n",
        "    return corrects.sum().float() / mask.sum().item()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A8fo_QTBxdx"
      },
      "source": [
        "# def create_mask(bret_inp, inp, targ):\n",
        "#     # encoder padding mask\n",
        "#     enc_padding_mask = create_padding_mask(inp,1)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
        "\n",
        "#     # decoder's first attention block(self-attention)\n",
        "#     # 使用的padding create_mask & look-ahead create_mask\n",
        "#     look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len] ##################\n",
        "#     dec_targ_padding_mask = create_padding_mask(targ,1)  # =>[b,1,1,targ_seq_len]\n",
        "#     combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # 结合了2种mask =>[b,1,targ_seq_len,targ_seq_len]\n",
        "\n",
        "#     # decoder's second attention block(encoder-decoder attention) 使用的padding create_mask\n",
        "#     # 【注意】：这里的mask是用于遮挡encoder output的填充pad，而encoder的输出与其输入shape都是[b,inp_seq_len,d_model]\n",
        "#     # 所以这里mask的长度是inp_seq_len而不是targ_mask_len\n",
        "#     dec_padding_mask = create_padding_mask(inp,1)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
        "\n",
        "#     return enc_padding_mask, combined_mask, dec_padding_mask ,create_padding_mask(inp,0)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bPHR9L2CJ6o"
      },
      "source": [
        "def validate_step(model, bretinp,inp, targ):\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "    tem = inp.shape[1]\n",
        "    bretinp = bretinp[:,:tem]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask,b_mask = create_mask(bretinp,inp, targ_inp)\n",
        "    bretinp = bretinp.to(device)\n",
        "    inp = inp.to(device)\n",
        "    b_mask = b_mask.to(device)\n",
        "\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "\n",
        "    model.eval()  \n",
        "\n",
        "    with torch.no_grad():\n",
        "        # forward\n",
        "        prediction, _ = model(bretinp,inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask,b_mask)\n",
        "    \n",
        "        \n",
        "        val_loss = mask_loss_func(targ_real, prediction,1)\n",
        "        val_metric = mask_accuracy_func(targ_real, prediction,1)\n",
        "        \n",
        "    return val_loss.item(), val_metric.item() \n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HVZxgUzB_Yg"
      },
      "source": [
        "def train_step(model,bretinp, inp, targ):\n",
        "    tem = inp.shape[1]\n",
        "    bretinp = bretinp[:,:tem]\n",
        "    targ_inp = targ[:, :-1]\n",
        "    targ_real = targ[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask,b_mask = create_mask(bretinp,inp, targ_inp)\n",
        "    b_mask = b_mask.to(device)\n",
        "    bretinp = bretinp.to(device)\n",
        "    inp = inp.to(device)\n",
        "    targ_inp = targ_inp.to(device)\n",
        "    targ_real = targ_real.to(device)\n",
        "    enc_padding_mask = enc_padding_mask.to(device)\n",
        "    combined_mask = combined_mask.to(device)\n",
        "    dec_padding_mask = dec_padding_mask.to(device)\n",
        "    model.train()  # 设置train mode\n",
        "    optimizer.zero_grad()  # 梯度清零\n",
        "\n",
        "    # forward\n",
        "    prediction, _ = model(bretinp,inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask,b_mask)\n",
        "    loss = mask_loss_func(targ_real, prediction,1)\n",
        "    metric = mask_accuracy_func(targ_real, prediction,1)\n",
        "\n",
        "    # backward\n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "\n",
        "    return loss.item(), metric.item()\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTpEoM6HCaoc"
      },
      "source": [
        "EPOCHS = 20 # 50 # 30  # 20\n",
        "\n",
        "print_trainstep_every = 50  # 每50个step做一次打印\n",
        "\n",
        "metric_name = 'acc'\n",
        "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name]) # 记录训练历史信息\n",
        "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name,'test_loss', 'test_' + metric_name, 'bleu' ])\n",
        "\n",
        "\n",
        "def printbar():\n",
        "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
        "    print('\\n' + \"==========\"*8 + '%s'%nowtime)\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLUZb_aCjDl"
      },
      "source": [
        "\n",
        "import timeit\n",
        "\n",
        "\n",
        "def train_model(model, epochs,datat,datav,datatest,datatest1,datatest2 ,print_every,BATCH_SIZE):\n",
        "    starttime = time.time()\n",
        "    print('*' * 27, 'start training...')\n",
        "    printbar()\n",
        "\n",
        "    best_acc = 0.\n",
        "    df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name,'test_loss', 'test_' + metric_name, 'test1_loss', 'test1_' + metric_name,'test2_loss', 'test2_' + metric_name])\n",
        "    for epoch in range(1,epochs+1):\n",
        "        # load data\n",
        "        val_dataloader =  torch.utils.data.DataLoader(datav, batch_size = 256 ,shuffle = False,collate_fn= collate_fn)\n",
        "        test_dataloader =  torch.utils.data.DataLoader(datatest, batch_size = 256 ,shuffle = False,collate_fn= collate_fn)\n",
        "        test_dataloader1 =  torch.utils.data.DataLoader(datatest1, batch_size = 256 ,shuffle = False,collate_fn= collate_fn)\n",
        "        test_dataloader2 =  torch.utils.data.DataLoader(datatest2, batch_size = 256 ,shuffle = False,collate_fn= collate_fn)\n",
        "        train_dataloader = torch.utils.data.DataLoader(datat, batch_size = BATCH_SIZE ,shuffle = True,collate_fn= collate_fn)\n",
        "        loss_sum = 0.\n",
        "        metric_sum = 0.\n",
        "        start = timeit.default_timer()\n",
        "        for step, (bretinp,inp, targ) in enumerate(train_dataloader,start = 1):\n",
        "\n",
        "            loss, metric = train_step(model,bretinp, inp, targ)\n",
        "\n",
        "            loss_sum += loss\n",
        "            metric_sum += metric\n",
        "\n",
        "            if step % print_every == 0:\n",
        "                print('*' * 8, f'[step = {step}] loss: {loss_sum / step:.3f}, {metric_name}: {metric_sum / step:.3f}')\n",
        "            \n",
        "        stop = timeit.default_timer()\n",
        "\n",
        "        print('Time: ', stop - start) \n",
        "        \n",
        "        scheduler.step()\n",
        "        val_loss_sum = 0.\n",
        "        val_metric_sum = 0.\n",
        "        for val_step, (bretinp,inp, targ) in enumerate(val_dataloader, start=1):\n",
        "            loss, metric = validate_step(model,bretinp, inp, targ)\n",
        "\n",
        "            val_loss_sum += loss\n",
        "            val_metric_sum += metric\n",
        "\n",
        "        test_loss_sum = 0.\n",
        "        test_metric_sum = 0.\n",
        "        bleu = []\n",
        "        for test_step, (bretinp,inp, targ) in enumerate(test_dataloader, start=1):\n",
        "            loss, metric = validate_step(model,bretinp, inp, targ)\n",
        "\n",
        "            test_loss_sum += loss\n",
        "            test_metric_sum += metric\n",
        "\n",
        "        test_loss_sum1 = 0.\n",
        "        test_metric_sum1 = 0.\n",
        "        bleu = []\n",
        "        for test_step1, (bretinp,inp, targ) in enumerate(test_dataloader1, start=1):\n",
        "            # inp [64, 10] , targ [64, 10]\n",
        "            loss, metric = validate_step(model,bretinp, inp, targ)\n",
        "\n",
        "            test_loss_sum1 += loss\n",
        "            test_metric_sum1 += metric\n",
        "        test_loss_sum2 = 0.\n",
        "        test_metric_sum2 = 0.\n",
        "        bleu = []\n",
        "        for test_step2, (bretinp,inp, targ) in enumerate(test_dataloader2, start=1):\n",
        "            # inp [64, 10] , targ [64, 10]\n",
        "            loss, metric = validate_step(model,bretinp, inp, targ)\n",
        "\n",
        "            test_loss_sum2 += loss\n",
        "            test_metric_sum2 += metric\n",
        "        if epoch<1000:\n",
        "          record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step,\n",
        "                  test_loss_sum/test_step, test_metric_sum/test_step,test_loss_sum1/test_step1, test_metric_sum1/test_step1,test_loss_sum2/test_step2, test_metric_sum2/test_step2)\n",
        "          df_history.loc[epoch - 1] = record\n",
        "          print('EPOCH = {} loss: {:.3f}, {}: {:.3f}, val_loss: {:.3f}, val_{}: {:.3f},test_loss: {:.3f}, test_{}: {:.3f}, test1_loss: {:.3f}, test1_{}: {:.3f},test2_loss: {:.3f}, test2_{}: {:.3f}'.format(\n",
        "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4],record[5],metric_name, record[6],record[7],metric_name,record[8],record[9],metric_name,record[10]))\n",
        "          printbar()\n",
        "          if len(df_history)>=11:\n",
        "            if record[3]>=df_history.loc[epoch - 11][3]:\n",
        "              break\n",
        "          else:\n",
        "            continue\n",
        "        \n",
        "    print('finishing training...')\n",
        "    endtime = time.time()\n",
        "    time_elapsed = endtime - starttime\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    return df_history\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zf6tNjsG5Ew"
      },
      "source": [
        "\n",
        "\n",
        "def tokenzier_decode1(sentence_ids, vocab):\n",
        "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
        "    # print(sentence)\n",
        "    return \" \".join(sentence[1:])\n",
        "\n",
        "def evaluate(model, inp_sentence):\n",
        "    model.eval()  # 设置eval mode\n",
        "    # print(tokenizer(inp_sentence))\n",
        "    inp = [SRC_TEXT.vocab.stoi['[CLS]']] + [SRC_TEXT.vocab.stoi[i] for i in tz.tokenize(inp_sentence)] +[SRC_TEXT.vocab.stoi['[SEP]']]\n",
        "    bretinp =  torch.tensor([tz.convert_tokens_to_ids(['[CLS]'] + tz.tokenize(inp_sentence) + ['[SEP]'])]) # 转化为索引\n",
        "    # tem = len(bretinp[0])\n",
        "    # inp = [SRC_TEXT.vocab.stoi['[CLS]']]+[SRC_TEXT.vocab.stoi[i] for i in inp_sentence.split()]+[SRC_TEXT.vocab.stoi['[SEP]']]\n",
        "    # inp += (tem-len(inp))*[0]\n",
        "    \n",
        "    inp = torch.unsqueeze(torch.tensor(inp),0)\n",
        "    \n",
        "    decoder_input = [TARG_TEXT.vocab.stoi['[CLS]']]\n",
        "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[b=1,seq_len=1]\n",
        "    # print(decoder_input.shape)\n",
        "    m = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask,b_mask = create_mask(bretinp.cpu(),inp.cpu(), decoder_input.cpu())\n",
        "        bretinp = bretinp.to(device)  \n",
        "        inp = inp.to(device)\n",
        "        enc_padding_mask = enc_padding_mask.to(device)\n",
        "        enout = model.encoder(inp, enc_padding_mask) \n",
        "        bertout = model.bert(bretinp)[0][-1]\n",
        "        for i in range(MAX_LENGTH + 2):\n",
        "            # enc_padding_mask, combined_mask, dec_padding_mask,b_mask = create_mask(inp_sentence_ids.cpu(), decoder_input.cpu()) ################\n",
        "            # [b,1,1,inp_seq_len], [b,1,targ_seq_len,inp_seq_len], [b,1,1,inp_seq_len]\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask,b_mask = create_mask(bretinp.cpu(),inp.cpu(), decoder_input.cpu())\n",
        "            b_mask = b_mask.to(device)\n",
        "            \n",
        "            decoder_input = decoder_input.to(device)\n",
        "            \n",
        "            combined_mask = combined_mask.to(device)\n",
        "            dec_padding_mask = dec_padding_mask.to(device)\n",
        "\n",
        "          # bretinp,inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask,b_mask\n",
        "            predictions, attention_weights = model.decoder(decoder_input, enout, combined_mask, dec_padding_mask,bertout,b_mask)\n",
        "           \n",
        "            predictions = model.final_layer(predictions)\n",
        "            prediction = m(predictions[:, -1:, :])  # =>[b=1, 1, target_vocab_size]\n",
        "            \n",
        "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
        "            # print('prediction_id:', prediction_id, prediction_id.dtype) # torch.int64\n",
        "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['[SEP]']:\n",
        "                return decoder_input.squeeze(dim=0), attention_weights\n",
        "\n",
        "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
        "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
        "\n",
        "\n",
        "    return decoder_input.squeeze(dim=0), attention_weights"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnFzAdyOCt7f",
        "outputId": "09702ed1-4ebe-49f1-a854-2549d36f009c"
      },
      "source": [
        "# datat = list(zip(enco,final_trains))\n",
        "datat = list(zip(enco,final_trains_i,final_trains))\n",
        "# datav = list(zip(encoval,final_val_s))\n",
        "datav = list(zip(encoval,final_val_i,final_val_s))\n",
        "# datatest = list(zip(encotest,final_test_s))\n",
        "datatest = list(zip(encotest,final_test_i,final_test_s))\n",
        "datatest1 = list(zip(encotest1,final_test_i1,final_test_s))\n",
        "datatest2 = list(zip(encotest2,final_test_i2,final_test_s))\n",
        "num_layers = 4\n",
        "d_model = 768\n",
        "dff =  800\n",
        "num_heads = 6\n",
        "\n",
        "\n",
        "\n",
        "input_vocab_size = len(SRC_TEXT.vocab)\n",
        "target_vocab_size = len(TARG_TEXT.vocab) # 2591\n",
        "dropout_rate = 0.25\n",
        "print(target_vocab_size)\n",
        "torch.cuda.empty_cache()\n",
        "dual_transformer = Transformer(num_layers,\n",
        "                          d_model,\n",
        "                          num_heads,\n",
        "                          dff,\n",
        "                           input_vocab_size,\n",
        "                          target_vocab_size,\n",
        "             input_vocab_size,\n",
        "                          pe_target=target_vocab_size,\n",
        "                          \n",
        "                          rate=dropout_rate)\n",
        "dual_transformer = dual_transformer.to(device)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(dual_transformer.parameters(),lr=0.0003)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer , 3,\n",
        "                gamma =0.8)\n",
        "df_history = train_model(dual_transformer, 20, datat,datav,datatest,datatest1,datatest2,100,256)\n",
        "\n",
        "print(df_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19063\n",
            "*************************** start training...\n",
            "\n",
            "================================================================================2021-09_26 19:42:04\n",
            "******** [step = 100] loss: 2.392, acc: 0.653\n",
            "******** [step = 200] loss: 1.962, acc: 0.732\n",
            "Time:  140.17464453799994\n",
            "EPOCH = 1 loss: 1.954, acc: 0.733, val_loss: 1.541, val_acc: 0.815,test_loss: 1.551, test_acc: 0.814, test1_loss: 1.627, test1_acc: 0.802,test2_loss: 1.708, test2_acc: 0.789\n",
            "\n",
            "================================================================================2021-09_26 19:45:35\n",
            "******** [step = 100] loss: 1.301, acc: 0.839\n",
            "******** [step = 200] loss: 1.231, acc: 0.849\n",
            "Time:  139.957120756\n",
            "EPOCH = 2 loss: 1.228, acc: 0.849, val_loss: 1.158, val_acc: 0.861,test_loss: 1.167, test_acc: 0.860, test1_loss: 1.301, test1_acc: 0.844,test2_loss: 1.435, test2_acc: 0.827\n",
            "\n",
            "================================================================================2021-09_26 19:49:06\n",
            "******** [step = 100] loss: 1.185, acc: 0.826\n",
            "******** [step = 200] loss: 1.021, acc: 0.858\n",
            "Time:  140.15060395\n",
            "EPOCH = 3 loss: 1.017, acc: 0.858, val_loss: 0.884, val_acc: 0.890,test_loss: 0.889, test_acc: 0.890, test1_loss: 1.066, test1_acc: 0.869,test2_loss: 1.241, test2_acc: 0.848\n",
            "\n",
            "================================================================================2021-09_26 19:52:37\n",
            "******** [step = 100] loss: 0.656, acc: 0.910\n",
            "******** [step = 200] loss: 0.619, acc: 0.915\n",
            "Time:  140.09659813400003\n",
            "EPOCH = 4 loss: 0.618, acc: 0.915, val_loss: 0.657, val_acc: 0.917,test_loss: 0.661, test_acc: 0.915, test1_loss: 0.880, test1_acc: 0.890,test2_loss: 1.100, test2_acc: 0.864\n",
            "\n",
            "================================================================================2021-09_26 19:56:08\n",
            "******** [step = 100] loss: 0.440, acc: 0.935\n",
            "******** [step = 200] loss: 0.415, acc: 0.939\n",
            "Time:  139.63574077399994\n",
            "EPOCH = 5 loss: 0.415, acc: 0.939, val_loss: 0.501, val_acc: 0.936,test_loss: 0.503, test_acc: 0.936, test1_loss: 0.756, test1_acc: 0.906,test2_loss: 1.009, test2_acc: 0.877\n",
            "\n",
            "================================================================================2021-09_26 19:59:39\n",
            "******** [step = 100] loss: 0.281, acc: 0.957\n",
            "******** [step = 200] loss: 0.270, acc: 0.958\n",
            "Time:  140.28810864700017\n",
            "EPOCH = 6 loss: 0.269, acc: 0.958, val_loss: 0.405, val_acc: 0.949,test_loss: 0.405, test_acc: 0.948, test1_loss: 0.679, test1_acc: 0.916,test2_loss: 0.952, test2_acc: 0.885\n",
            "\n",
            "================================================================================2021-09_26 20:03:10\n",
            "******** [step = 100] loss: 0.170, acc: 0.975\n",
            "******** [step = 200] loss: 0.163, acc: 0.976\n",
            "Time:  139.96907304900014\n",
            "EPOCH = 7 loss: 0.163, acc: 0.976, val_loss: 0.347, val_acc: 0.956,test_loss: 0.348, test_acc: 0.956, test1_loss: 0.638, test1_acc: 0.922,test2_loss: 0.931, test2_acc: 0.890\n",
            "\n",
            "================================================================================2021-09_26 20:06:41\n",
            "******** [step = 100] loss: 0.107, acc: 0.985\n",
            "******** [step = 200] loss: 0.105, acc: 0.986\n",
            "Time:  139.89538706299982\n",
            "EPOCH = 8 loss: 0.104, acc: 0.986, val_loss: 0.315, val_acc: 0.960,test_loss: 0.313, test_acc: 0.960, test1_loss: 0.614, test1_acc: 0.925,test2_loss: 0.919, test2_acc: 0.893\n",
            "\n",
            "================================================================================2021-09_26 20:10:12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJXqrXiiGv29"
      },
      "source": [
        "all_re = []\n",
        "gt= [[i[1].split()]for i in test_pairs]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "        pred_result, _ = evaluate(transformer1, test_pairs[i][0])\n",
        "        candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "        sbs = sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "        all_re.append(sbs)\n",
        "        total+= sbs\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQIl6C2fwOHO"
      },
      "source": [
        "\n",
        "gt= [[i[1].split()]for i in test_pairs1]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs1))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "        pred_result, _ = evaluate(transformer1, test_pairs1[i][0])\n",
        "        candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zt2NpaCwPMD"
      },
      "source": [
        "\n",
        "gt= [[i[1].split()]for i in test_pairs2]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs2))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "        pred_result, _ = evaluate(transformer1, test_pairs2[i][0])\n",
        "        candidates = tokenzier_decode1(pred_result, TARG_TEXT.vocab).split()\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAYz3Cv1aReL"
      },
      "source": [
        "def batch_translate(pairs,n):\n",
        "    for i in range(n):\n",
        "        pair = np.random.choices(pairs)\n",
        "        print('input:', pair[0])\n",
        "        print('target:', pair[1])\n",
        "        pred_result, _ = evaluate(transformer1, pair[0])\n",
        "        pred_sentence = tokenzier_decode1(pred_result, TARG_TEXT.vocab)\n",
        "        print('pred:', pred_sentence)\n",
        "        print('')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7NK5-ulpDG"
      },
      "source": [
        "batch_translate(test_pairs,10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}