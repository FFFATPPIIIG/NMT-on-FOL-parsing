{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline_batch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j5PiYBLqMwtv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeLCkRK-eYKA",
        "outputId": "ec5608fa-cb9a-48a5-c484-c02d3bbfd777"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torchtext\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMjaFDmCKj7e"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ppp6QeOed3y"
      },
      "source": [
        "\n",
        "p_intent = []\n",
        "p_snippet = []\n",
        "c = 0\n",
        "iit = []\n",
        "# train data\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      # for removing repeated data\n",
        "      if i[:-1].lower() not in p_intent:\n",
        "        iit.append(c)\n",
        "      c+=1\n",
        "      p_intent.append(i[:-1].lower())\n",
        "p_intentn = []\n",
        "# if load noise training set\n",
        "# with open('x25/x50/x75/x100.txt', 'r',encoding='UTF-8') as intent:\n",
        "#     for i in intent:\n",
        "#       p_intentn.append(i[:-1].lower())\n",
        "with open('x.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentn.append(i[:-1].lower())\n",
        "\n",
        "with open('y.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippet.append(i.lower())\n",
        "\n",
        "train_pairs = [[p_intentn[i],p_snippet[i]] for i in iit]\n",
        "# validation data\n",
        "p_intentval = []\n",
        "p_snippetval = []\n",
        "iiv = []\n",
        "c= 0\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      # for removing repeated data and data already in training data\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intentval:\n",
        "        iiv.append(c)\n",
        "      \n",
        "      p_intentval.append(i[:-1].lower())\n",
        "      c+=1\n",
        "p_intentvaln = []\n",
        "with open('xv.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      p_intentvaln.append(i[:-1].lower())\n",
        "      c+=1\n",
        "with open('yv.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      #i = i.split(' ')\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippetval.append(i.lower())\n",
        "val_pairs = [[p_intentvaln[i],p_snippetval[i]] for i in iiv]\n",
        "#  test data\n",
        "p_intenttest = []\n",
        "p_snippettest = []\n",
        "ii = []\n",
        "c= 0\n",
        "with open('xt.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      # for removing repeated data and data already in training data\n",
        "      if i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest:\n",
        "        ii.append(c)\n",
        "      c+=1\n",
        "      p_intenttest.append(i[:-1].lower())\n",
        "with open('yt.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      p_snippettest.append(i.lower())\n",
        "test_pairs = [[p_intenttest[i],p_snippettest[i]] for i in ii]\n",
        "# lv1 noise test data\n",
        "p_intenttest1 = []\n",
        "p_snippettest1 = []\n",
        "with open('xt1.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest1.append(i[:-1].lower())\n",
        "with open('yt1.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest1.append(i.lower())\n",
        "test_pairs1 = [[p_intenttest1[i],p_snippettest1[i]] for i in ii]\n",
        "# lv2 noise test data\n",
        "p_intenttest2 = []\n",
        "p_snippettest2 = []\n",
        "with open('xt2.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      \n",
        "      p_intenttest2.append(i[:-1].lower())\n",
        "with open('yt2.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      p_snippettest2.append(i.lower())\n",
        "test_pairs2 = [[p_intenttest2[i],p_snippettest2[i]] for i in ii]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Ou4kaHWRCQ"
      },
      "source": [
        "# if load extra training data\n",
        "# p_intentextra = []\n",
        "# p_snippetextra = []\n",
        "# c = 0\n",
        "# iite = []\n",
        "# with open('xextra.txt', 'r',encoding='UTF-8') as intent:\n",
        "#     for i in intent:\n",
        "#       if i[:-1].lower() not in p_intentextra and i[:-1].lower() not in p_intent and i[:-1].lower() not in p_intenttest and i[:-1].lower() not in p_intentval:\n",
        "#         iite.append(c)\n",
        "#       c+=1\n",
        "#       p_intentextra.append(i[:-1].lower())\n",
        "\n",
        "# with open('yextra.txt', 'r',encoding='UTF-8') as snippet:\n",
        "#     for i in snippet:\n",
        "#       p_snippetextra.append(i[:-1].lower())\n",
        "# extratrain = [[p_intentextra[i],p_snippetextra[i]] for i in iite]\n",
        "# train_pairs = train_pairs+extratrain"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjTBV1IMjIa"
      },
      "source": [
        "## Pre-processing and building vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djTMG8o2g8pH"
      },
      "source": [
        "\n",
        "SRC_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                              \n",
        "                                )\n",
        "TARG_TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                                 )\n",
        "                                 "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1naEGHkjuw7"
      },
      "source": [
        "ptrainx = [['sos']+x[0].split()+['eos'] for x in train_pairs]\n",
        "ptrainy = [['sos']+x[1].split()+['eos'] for x in train_pairs]\n",
        "pvalx = [['sos']+x[0].split()+['eos'] for x in val_pairs]\n",
        "pvaly = [['sos']+x[1].split()+['eos'] for x in val_pairs]\n",
        "ptestx = [['sos']+x[0].split()+['eos'] for x in test_pairs]\n",
        "ptesty = [['sos']+x[1].split()+['eos'] for x in test_pairs]\n",
        "ptestx1 = [['sos']+x[0].split()+['eos'] for x in test_pairs1]\n",
        "ptesty1 = [['sos']+x[1].split()+['eos'] for x in test_pairs1]\n",
        "ptestx2 = [['sos']+x[0].split()+['eos'] for x in test_pairs2]\n",
        "ptesty2 = [['sos']+x[1].split()+['eos'] for x in test_pairs2]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT2bkq24jKHa",
        "outputId": "7071a2c1-9afd-4dd9-f279-00bdd30dfd1c"
      },
      "source": [
        "SRC_TEXT.build_vocab(ptrainx) \n",
        "print(len(SRC_TEXT.vocab))\n",
        "print(SRC_TEXT.vocab.itos[0])\n",
        "print(SRC_TEXT.vocab.itos[1])\n",
        "print(SRC_TEXT.vocab.itos[2])\n",
        "print(SRC_TEXT.vocab.itos[3])\n",
        "print(SRC_TEXT.vocab.stoi['sos'])\n",
        "print(SRC_TEXT.vocab.stoi['eos'])\n",
        "\n",
        "TARG_TEXT.build_vocab(ptrainy)\n",
        "print(len(TARG_TEXT.vocab))\n",
        "print(TARG_TEXT.vocab.itos[0])\n",
        "print(TARG_TEXT.vocab.itos[1])\n",
        "print(TARG_TEXT.vocab.itos[2])\n",
        "print(TARG_TEXT.vocab.itos[3])\n",
        "print(TARG_TEXT.vocab.stoi['sos'])\n",
        "print(TARG_TEXT.vocab.stoi['eos'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18911\n",
            "<unk>\n",
            "<pad>\n",
            "eos\n",
            "sos\n",
            "3\n",
            "2\n",
            "19063\n",
            "<unk>\n",
            "<pad>\n",
            "(\n",
            ")\n",
            "6\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ1HVyjzkLVn"
      },
      "source": [
        "final_trainx = [torch.tensor([SRC_TEXT.vocab.stoi[x] for x in i]) for i in ptrainx]\n",
        "final_trainy = [torch.tensor([TARG_TEXT.vocab.stoi[x] for x in i]) for i in ptrainy]\n",
        "final_valx = [torch.tensor([SRC_TEXT.vocab.stoi[x] for x in i]) for i in pvalx]\n",
        "final_valy = [torch.tensor([TARG_TEXT.vocab.stoi[x] for x in i]) for i in pvaly]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqei5F3-nWGC"
      },
      "source": [
        "final_testx = [torch.tensor([SRC_TEXT.vocab.stoi[x] for x in i]) for i in ptestx]\n",
        "final_testy = [torch.tensor([TARG_TEXT.vocab.stoi[x] for x in i]) for i in ptesty]\n",
        "final_testx1 = [torch.tensor([SRC_TEXT.vocab.stoi[x] for x in i]) for i in ptestx1]\n",
        "final_testy1 = [torch.tensor([TARG_TEXT.vocab.stoi[x] for x in i]) for i in ptesty1]\n",
        "final_testx2 = [torch.tensor([SRC_TEXT.vocab.stoi[x] for x in i]) for i in ptestx2]\n",
        "final_testy2 = [torch.tensor([TARG_TEXT.vocab.stoi[x] for x in i]) for i in ptesty2]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5PiYBLqMwtv"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ARtLvvnYhB"
      },
      "source": [
        "data = list(zip(final_trainx,final_trainy))\n",
        "datav = list(zip(final_valx,final_valy))\n",
        "datat = list(zip(final_testx,final_testy))\n",
        "datat1 = list(zip(final_testx1,final_testy1))\n",
        "datat2 = list(zip(final_testx2,final_testy2))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uso8-DPtnsw5"
      },
      "source": [
        "# for padding in the mini-batch\n",
        "def collate_fn(data):\n",
        "    inputs,targ = zip(*data)\n",
        "    seq_len = torch.tensor([len(i) for i in inputs])\n",
        "    inputs= torch.nn.utils.rnn.pad_sequence(inputs, padding_value=1)\n",
        "    targ = torch.nn.utils.rnn.pad_sequence(targ, padding_value=1)\n",
        "\n",
        "    return inputs,targ,seq_len\n",
        "d = torch.utils.data.DataLoader(datat, batch_size = 64 ,shuffle = False,collate_fn= collate_fn)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw858fGIvC4c",
        "outputId": "a89053cf-76b4-4059-e1fe-5f3d1a1a066a"
      },
      "source": [
        "for i,(inp,tar,h) in enumerate(d):\n",
        "  print(tar)\n",
        "  print([SRC_TEXT.vocab.itos[i] for i in inp[:,1]])\n",
        "  print(tar.size(1))\n",
        "  break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
            "             6,     6,     6,     6],\n",
            "        [   13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
            "            13,    13,    13,    13],\n",
            "        [   11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11,    11,    11,    11,    11,    11,    11,\n",
            "            11,    11,    11,    11],\n",
            "        [ 2316,  4742,   324,   148,   381,  3640,   293,   662,    41,   436,\n",
            "           114,  1137,   127,  3772,   461,   367,   474,  5424,   152,  1374,\n",
            "             0,     0,   921,  1310,  1125,  1405,     0,  1857,   316,     0,\n",
            "           675,  2133,   470,  3801,   446,   845,  1895,   625,  6927,   131,\n",
            "           412,   228,    34,   393,  1136,  2852,   535,   339,   544,  4075,\n",
            "          1969,  7336,  7957,  1614,   598,  1359,   197,  1631,  1199, 18759,\n",
            "          1132,   690,  9476,   484],\n",
            "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2],\n",
            "        [    4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
            "             4,     4,     4,     4],\n",
            "        [    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3],\n",
            "        [    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
            "             5,     5,     5,     5]])\n",
            "['sos', 'there', 'does', 'not', 'exist', 'a', 'tail', 'eos']\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5dLwY42M9yU"
      },
      "source": [
        "## Encoder,Decoder, seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpXKzWr8rVYF"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size,embedding_size,hidden_size,num_layers,dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size,num_layers , bidirectional = True, dropout=dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.r = torch.nn.ReLU()\n",
        "    def forward(self, input,seq_length):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, seq_length,enforce_sorted=False)\n",
        "        output, hidden = self.gru(packed)\n",
        "        output,_ = nn.utils.rnn.pad_packed_sequence(output,batch_first=False,total_length=max(seq_length),)\n",
        "        # concat two hiddens to one hidden for decoder use\n",
        "        hidden = torch.cat((hidden[-1],hidden[-2]),dim=1)\n",
        "        hidden = self.r(self.fc(hidden)).unsqueeze(0)\n",
        "        return output,hidden\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHElwhVxrdl-"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size,embedding_size,hidden_size,output_size,num_layers,dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "        self.gru = nn.GRU(hidden_size*2+embedding_size, hidden_size,num_layers,dropout=dropout_rate)\n",
        "        self.attension = nn.Linear(hidden_size*3,1)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "    def forward(self, encoder_output ,input, hidden):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        h_attension = hidden.repeat(encoder_output.size(0),1,1)\n",
        "        # calculate attention\n",
        "        attention = self.relu(self.attension(torch.cat((h_attension,encoder_output),dim=2)))\n",
        "        attention = self.softmax(attention)\n",
        "        c = torch.bmm(attention.permute(1,2,0),encoder_output.permute(1,0,2)).permute(1,0,2)\n",
        "        embedded = torch.cat((embedded,c),dim=2)\n",
        "        # print(embedded.shape)\n",
        "        output,hidden = self.gru(embedded,hidden)\n",
        "        pred = self.out(output)\n",
        "        pred = pred.squeeze(0)\n",
        "        return pred,hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFX8AmtXr1j9"
      },
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder,decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, source,target,s,teacher_force_ratio=0.5,):\n",
        "        batch_size = source.size(1)\n",
        "        target_length = target.size(0)\n",
        "        target_vocab_size = len(TARG_TEXT.vocab)\n",
        "        output_encoder,hidden = self.encoder(source,s)\n",
        "        outputs = torch.zeros(target_length,batch_size,target_vocab_size).to(device)\n",
        "        x = target[0,:]\n",
        "        # print(hidden)\n",
        "        for t in range(1,target_length):\n",
        "          output,hidden = self.decoder(output_encoder,x,hidden)\n",
        "          ### output: batch*vocab_size\n",
        "          outputs[t] = output\n",
        "          pred_max = output.argmax(1)\n",
        "          x = target[t] if random.random() < teacher_force_ratio else pred_max\n",
        "        return outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqzPFug3si71"
      },
      "source": [
        "def train(datat,batch_s,model):\n",
        "  model.train()\n",
        "  d = torch.utils.data.DataLoader(datat, batch_size = batch_s ,shuffle = True,collate_fn= collate_fn)\n",
        "  loss_train = 0\n",
        "  acc_train = 0\n",
        "  for i, (x_train,x_label,s) in enumerate(d,start = 1):\n",
        "      optimizer.zero_grad()\n",
        "      x_train = x_train.to(device)\n",
        "      x_label = x_label.to(device)\n",
        "      predict = model(x_train,x_label,s)\n",
        "      pred_dim = predict.shape[-1]\n",
        "      x_label = x_label[1:].view(-1)\n",
        "      predict = predict[1:].view(-1, pred_dim)\n",
        "      # print(x_label.shape)\n",
        "      loss = criterion(predict, x_label)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "      _pred = predict.argmax(dim=-1)  \n",
        "      corrects = _pred.eq(x_label) \n",
        "      mask = torch.logical_not(x_label.eq(1)) \n",
        "      corrects *= mask\n",
        "      acc_train += corrects.sum().float() / mask.sum()\n",
        "  return loss_train/i,acc_train/i\n",
        "\n",
        "def val(datat,batch_s,model):\n",
        "  model.eval()\n",
        "  d = torch.utils.data.DataLoader(datat, batch_size = batch_s ,shuffle = False,collate_fn= collate_fn)\n",
        "  loss_val = 0\n",
        "  acc_val = 0\n",
        "  for i, (x_val,x_label,s) in enumerate(d,start = 1):\n",
        "      with torch.no_grad():\n",
        "        x_val = x_val.to(device)\n",
        "        x_label = x_label.to(device)\n",
        "        predict = model(x_val,x_label,s)\n",
        "        pred_dim = predict.shape[-1]\n",
        "        x_label = x_label[1:].view(-1)\n",
        "        predict = predict[1:].view(-1, pred_dim)\n",
        "\n",
        "        loss = criterion(predict, x_label)\n",
        "    \n",
        "        loss_val += loss.item()\n",
        "        _pred = predict.argmax(dim=-1)  # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
        "        corrects = _pred.eq(x_label) \n",
        "        mask = torch.logical_not(x_label.eq(1)) \n",
        "        corrects *= mask\n",
        "      # print(corrects.shape)\n",
        "      # print(x_label.shape)\n",
        "        acc_val += corrects.sum().float() / mask.sum()\n",
        "        # corrects *= mask\n",
        "      # print(mask.sum())\n",
        "      # print(corrects.shape)\n",
        "      # print(x_label.shape)\n",
        "        # acc_val += corrects.sum().float() / (x_label.shape[0])\n",
        "  return loss_val/i,acc_val/i"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5lleoY2waZE"
      },
      "source": [
        "def predict(sen,trg, model, source_vocab, target_vocab ):\n",
        "\n",
        "    # Convert each source token to integer values using the vocabulary\n",
        "    tokens =  ['sos']+[token.lower() for token in sen.split(' ')] + ['eos']\n",
        "    src_indexes = [source_vocab.stoi[token] for token in tokens]\n",
        "    # print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    # print(src_tensor.shape)\n",
        "    model.eval()\n",
        "\n",
        "    # Run the forward pass of the encoder\n",
        "    encoder_outputs, hidden = model.encoder(src_tensor,torch.tensor([src_tensor.shape[0]]))\n",
        "\n",
        "    # Take the integer value of <sos> from the target vocabulary.\n",
        "    trg_index = [target_vocab.stoi['sos']]\n",
        "    next_token = torch.LongTensor(trg_index).to(device)\n",
        "\n",
        "    # print(next_token)\n",
        "    trg_indexes = [trg_index[0]]\n",
        "\n",
        "    # outputs = []\n",
        "    with torch.no_grad():\n",
        "        # Use the hidden and cell vector of the Encoder and in loop\n",
        "        # run the forward pass of the OneStepDecoder until some specified\n",
        "        # step (say 50) or when <eos> has been generated by the model.\n",
        "        for i in range(200):\n",
        "            output, hidden = model.decoder(encoder_outputs, next_token, hidden)\n",
        "\n",
        "\n",
        "            # Take the most probable word\n",
        "            \n",
        "            next_token = output.argmax(1)\n",
        "\n",
        "            trg_indexes.append(next_token.item())\n",
        "\n",
        "            predicted = target_vocab.itos[next_token[0]]\n",
        "            # print(predicted)\n",
        "            if predicted == 'eos':\n",
        "                break\n",
        "            \n",
        "    predicted_words = [target_vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    return predicted_words[1:-1]\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YqMSQAZOlTc"
      },
      "source": [
        "## Model setting and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KEIuSiYr7iu",
        "outputId": "56183a52-fc42-4384-ed7f-cd47147ebee5"
      },
      "source": [
        "### model parameter\n",
        "input_size = len(SRC_TEXT.vocab)\n",
        "output_size = len(TARG_TEXT.vocab)\n",
        "encoder_embedding_size = 400\n",
        "decoder_embedding_size = 400\n",
        "hidden_size = 800\n",
        "number_layers = 1\n",
        "encoder_dropout_rate = 0.25\n",
        "decoder_dropout_rate = 0.25\n",
        "###\n",
        "enc = Encoder(input_size, encoder_embedding_size, hidden_size, number_layers, encoder_dropout_rate).to(device)\n",
        "dec = Decoder(output_size, decoder_embedding_size, hidden_size,output_size,number_layers, decoder_dropout_rate).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer , 8,\n",
        "                gamma =0.9)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B_HV9HpVrts"
      },
      "source": [
        "import timeit\n",
        "early_ = []\n",
        "for e in range(100):\n",
        "    # print('epoch: ' + str(e))\n",
        "    start = timeit.default_timer()\n",
        "    t = train(data,256,model)\n",
        "    stop = timeit.default_timer()\n",
        "    print('Time: ', stop - start) \n",
        "    v = val(datav,256,model)\n",
        "    test = val(datat,256,model)\n",
        "    test1,test2 = [0,0],[0,0]\n",
        "    test1 = val(datat1,256,model)\n",
        "    test2 = val(datat2,256,model)\n",
        "    prediction = []\n",
        "    b = [0]\n",
        "    # early stop\n",
        "    if len(early_)>10:\n",
        "      if v[0]>=early_[e-10]:\n",
        "        print('EPOCH = {} loss: {:.3f}, {}: {:.3f},val_loss: {:.3f}, val_{}: {:.3f},test_loss: {:.3f}, test_{}: {:.3f}, test1_loss: {:.3f},test1_{}: {:.3f},test2_loss: {:.3f},test2_{}: {:.3f}'.format(\n",
        "            e, t[0], \"acc\", t[1], v[0], \"acc\", v[1],test[0], \"acc\", test[1],test1[0], \"acc\", test1[1],test2[0], \"acc\", test2[1]))\n",
        "        break\n",
        "    early_.append(v[0])\n",
        "    scheduler.step()\n",
        "    print('EPOCH = {} loss: {:.3f}, {}: {:.3f},val_loss: {:.3f}, val_{}: {:.3f},test_loss: {:.3f}, test_{}: {:.3f}, test1_loss: {:.3f},test1_{}: {:.3f},test2_loss: {:.3f},test2_{}: {:.3f}'.format(\n",
        "            e, t[0], \"acc\", t[1], v[0], \"acc\", v[1],test[0], \"acc\", test[1],test1[0], \"acc\", test1[1],test2[0], \"acc\", test2[1]))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRQuL7uDO47d"
      },
      "source": [
        "# calcualte BLEU for test set\n",
        "all_re = []\n",
        "gt= [[i[1].split()]for i in test_pairs]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "        \n",
        "        candidates = predict(test_pairs[i][0],test_pairs[i][1], model, SRC_TEXT.vocab, TARG_TEXT.vocab)\n",
        "\n",
        "        sbs = sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "        all_re.append(sbs)\n",
        "        total+=sbs\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNnke31ORW1v"
      },
      "source": [
        "# calcualte BLEU for lv1 noise test set\n",
        "gt= [[i[1].split()]for i in test_pairs1]\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        candidates = predict(test_pairs1[i][0],test_pairs1[i][1], model, SRC_TEXT.vocab, TARG_TEXT.vocab)\n",
        "\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftHZstjXDyCp"
      },
      "source": [
        "# calcualte BLEU for lv2 noise test set\n",
        "gt= [[i[1].split()]for i in test_pairs2]\n",
        "total = 0\n",
        "for i in tqdm(range(len(test_pairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        candidates = predict(test_pairs2[i][0],test_pairs2[i][1], model, SRC_TEXT.vocab, TARG_TEXT.vocab)\n",
        "\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChVh7h94tqp0"
      },
      "source": [
        "# calcualte BLEU for negation test set\n",
        "ngx = []\n",
        "ngy = []\n",
        "with open('xneg.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      ngx.append(i[:-1].lower())\n",
        "with open('yneg.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      ngy.append(i.lower())\n",
        "ngpairs = [[ngx[i],ngy[i]] for i in range(len(ngy))]\n",
        "gt= [[i[1].split()]for i in ngpairs]\n",
        "total = 0\n",
        "for i in tqdm(range(len(ngpairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        candidates = predict(ngpairs[i][0],ngpairs[i][1], model, SRC_TEXT.vocab, TARG_TEXT.vocab)\n",
        "\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(ngpairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dailrsRrt5gu"
      },
      "source": [
        "# calcualte BLEU for non-negation test set\n",
        "nongx = []\n",
        "nongy = []\n",
        "with open('xnoneg.txt', 'r',encoding='UTF-8') as intent:\n",
        "    for i in intent:\n",
        "      nongx.append(i[:-1].lower())\n",
        "with open('ynoneg.txt', 'r',encoding='UTF-8') as snippet:\n",
        "    for i in snippet:\n",
        "      i = i[:-1]\n",
        "      if i[-1]==' ':\n",
        "        i=i[:-1]\n",
        "      # if i.lower() not in p_snippet:\n",
        "      nongy.append(i.lower())\n",
        "nongpairs = [[nongx[i],nongy[i]] for i in range(len(nongy))]\n",
        "gt= [[i[1].split()]for i in nongpairs]\n",
        "total = 0\n",
        "for i in tqdm(range(len(nongpairs))):\n",
        "       \n",
        "      \n",
        "        references = gt[i]\n",
        "      \n",
        "        candidates = predict(nongpairs[i][0],nongpairs[i][1], model, SRC_TEXT.vocab, TARG_TEXT.vocab)\n",
        "\n",
        "\n",
        "\n",
        "        total+=sentence_bleu(references, candidates,smoothing_function=smoothie)\n",
        "total*100/len(nongpairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKV-FgQjLWgv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}